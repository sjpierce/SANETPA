---
title: Research Questions 2 and 3 Stopping-Ratio Models for SANE Training 
       Program Data
subtitle: | 
          | CSTAT Case: C1788
          | Clients: Rebecca Campbell, Autumn Ashley, & Katherine Dontje
author: 
  - name: Steven J. Pierce
    orcid: 0000-0002-0679-3019
    email: pierces1@msu.edu
    affil-id: 1 
affiliations: 
  - id: 1 
    name: Michigan State University
    department: Center for Statistical Training and Consulting
    url: "[https://cstat.msu.edu](https://cstat.msu.edu)"
bibliography: references.bib          # File holds BibTeX data for references
csl: apa.csl                          # File controls citation & reference list format
params:                               # Default values for parameters
  SourceDir: "scripts/"               # Relative path to folder holding this file
  SourceFile: "RQ2_RQ3_SR_Models.qmd"          # Name of this script file
  LogFile: "RQ2_RQ3_SR_Models_Draft.pdf"       # Name of rendered output file
  DHARMa.seed1: 6347
  DHARMa.seed2: 9654
  DHARMa.seed3: 9654
  DHARMa.seed4: 3321
  DHARMa.seed5: 1456
format: 
  pdf:                                # Settings for PDF output
    output-file: "RQ2_RQ3_SR_Models_Draft.pdf" # Default output file name 
    output-ext: "pdf"                 # Default file name extension
    documentclass: scrartcl           # LaTeX document type
    papersize: letter
    geometry:                         # Control page margins
      - top=1in
      - bottom=1in
      - left=1in
      - right=1in
    number-sections: true           # Auto-number each section
    toc: true                       # Include table of contents (TOC)
    toc-depth: 3                    # Number of layers in TOC
    colorlinks: true
    template-partials: 
      - title.tex   # Controls author affiliation formatting
    include-in-header:
      - file: compact-title.tex    # Controls spacing around title. 
      - text: |
          \usepackage{fancyhdr}
          \usepackage[noblocks]{authblk}
          \renewcommand*{\Authsep}{, }
          \renewcommand*{\Authand}{, }
          \renewcommand*{\Authands}{, }
          \renewcommand\Affilfont{\small}
          \usepackage[yyyymmdd,hhmmss]{datetime}    %% for currenttime command
          \usepackage{lastpage}                     %% For pageref command
          \usepackage{fontspec}
          \defaultfontfeatures{Ligatures=TeX}
          \usepackage[font={small}, margin=1cm, skip=2pt]{caption}
          \usepackage{url}
          \usepackage{floatrow}                     %% For controlling float placement
          \floatsetup[table]{capposition=top}       %% Puts table caption at top.
          \floatsetup[longtable]{margins=centering} %% Centers longtable tables
          \floatplacement{figure}{!ht}              %% Control placement of figures
          \floatplacement{table}{!ht}               %% Control placement of tables
          \usepackage[labelfont=bf, textfont=bf]{caption}  %% For bolded table/figure captions
          \usepackage{placeins}                     %% For FloatBarrier command
          \usepackage{booktabs}                     %% Used by kableExtra
          \usepackage{longtable}                    %% Used by kableExtra
          \usepackage{array}                        %% Used by kableExtra
          \usepackage{multirow}                     %% Used by kableExtra
          \usepackage{wrapfig}                      %% Used by kableExtra
          \usepackage{colortbl}                     %% Used by kableExtra
          \usepackage{pdflscape}                    %% Used by kableExtra
          \usepackage{tabu}                         %% Used by kableExtra
          \usepackage[normalem]{ulem}               %% Used by kableExtra
          \usepackage{makecell}                     %% Used by kableExtra
          \usepackage{xcolor}                       %% Used by kableExtra
          \usepackage{dcolumn}                      %% Used by kableExtra
          \usepackage{threeparttable}               %% Used by kableExtra
          \usepackage{threeparttablex}              %% Used by kableExtra
          \usepackage{amsmath}                      %% for equation support. 
          \usepackage{titling}      
          \usepackage{verbatim}                     %% For comment command
          \pretitle{\begin{center}\LARGE\bfseries\sffamily}
          \posttitle{\end{center}}
          \pagestyle{fancy}
          \lhead{SANETPA RQ2 and RQ3 SR Models}
          \chead{\includegraphics[height=0.85cm]{../graphics/Combomark-Horiz_Pantone-567.eps}}
          \rhead{\today\ \currenttime}
          \cfoot{ }
          \fancyfoot[R]{\thepage\ of \pageref*{LastPage}}
          \renewcommand{\headrulewidth}{0.4pt}
          \renewcommand{\footrulewidth}{0.4pt}
          \fancypagestyle{plain}{\pagestyle{fancy}}
          \newcommand*\tocentryformat[1]{{\sffamily#1}}  %% Fix TOC font style
          \RedeclareSectionCommands                      %% Fix TOC font style
            [
              tocentryformat=\tocentryformat,
              tocpagenumberformat=\tocentryformat
            ]
            {section,subsection,subsubsection,paragraph,subparagraph}
execute:                              # Default Quarto chunk execution options
  eval: true
  echo: fenced                        # Show the code in the output file
  output: true
  warning: true
  error: true
  include: true
knitr:                                # Default R knitr package chunk options
  opts_chunk: 
    message: true
    cfsize: "tiny" 
---

\lfoot{\texttt{\small \detokenize{`r params$LogFile`}}}

\FloatBarrier

# Purpose
This file is part of a research compendium [@Pierce-RN8756] associated with a 
study about a sexual assault nurse examiner training program [@Dontje-RN8757]. 
The study aims to document rates of attrition at three threshold points during
the program and understand predictors of attrition at each of those points.

This file reads an R data file created by another script in this compendium,
documents some methodology decisions and details, then runs the
stopping-ratio models that comprise the analyses answering research questions 
RQ2 and RQ3 for the study. This file also contains some narrative interpretation 
of the results and supplementary output such as tables and graphs derived from 
the models.

# Research Questions
The research questions addressed in this file may be briefly stated as follows:

* **RQ2.** What are the attrition rates from enrollment to completion of the 
  CSW, and when (i.e., at which module[s]) did participants commonly attrit?  
* **RQ3.** Do participants’ background characteristics, motivations for seeking 
  training, potential barriers, and emotional readiness for this work predict 
  attrition rates? 

::: {.callout-tip}
This document uses stopping-ratio models to answer both RQ2 and RQ3. 
:::

::: {.callout-note}
The investigators and the program staff have explicitly decided that the 
following potential predictors are not of substantive interest and should not be
pursued in this paper.

* Trainee's learning scores in the didactic training and clinical skills 
  workshop are not of interest. 
* Trainee demographics (sex, age, race, etc.) are not of interest because the 
  team wants to focus on analyzing predictors that have more intrinsic meaning 
  and for which there are theoretical reasons to expect effects on attrition. 

:::

\FloatBarrier

# Setup
This section documents some setup tasks that are useful to the statistician on 
the team. Most readers of this document will probably want skip directly to 
@sec-SR-Modeling.

\FloatBarrier

## Define Global Options
Global R chunk options are defined in the YAML header but local chunk options
will over-ride global options. We can temporarily disable an individual chunk by
inserting `#| eval: false` on a line at the top of the chunk. The method for
creating a `cfsize` option that controls font size in code chunks and their text
output is based on an answer to a question posted on
[stackoverflow.com](https://stackoverflow.com/a/46526740).

``` {r}
#| label: global-options

# Create a custom chunk hook/option for controlling font size in chunk & output.
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$cfsize != "normalsize", 
         paste0("\n \\", options$cfsize,"\n\n", x, "\n\n \\normalsize"), 
         x)
  })
```

\FloatBarrier

## Load Packages
R packages usually add new functions to the base R software, allowing you to do 
more things. Here, we load the specific R packages required for this script to 
work.

```{r}
#| label: load-packages
library(here)           # for here(), i_am(), makes code more portable.
library(devtools)       # for session_info()
library(rmarkdown)      # for pandoc_version()
library(knitr)          # for kable()
library(dplyr)          # for %>%, filter(), group_by(), mutate(), rename(), etc.
library(tidyverse)      # for map_dfr(), map_chr(), rowid_to_column(), 
                        # rownames_to_column()
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)     # for kable_styling(), add_header_above(), 
                        # column_spec(), row_spec() etc. 
library(kableExtra)     # for add_header_above(), footnote(), kable_styling()
library(broom)          # for glance(), tidy()
library(car)            # for vif()
library(DHARMa)         # for plotQQunif(), plotResiduals(), simulateResiduals(), 
                        # testDispersion()
library(emmeans)        # for emmeans()
library(performance)    # for model_performance(), r2()
library(scales)         # for label_number()
library(piercer)        # for brier(), display_num(), file_details(), 
                        # git_report(), R2Dev()
library(quarto)         # for quarto_version()
library(SANETPA)        # for version info 
```

\FloatBarrier

## Declare Path
This next chunk declares the path to this script relative to the project-level 
root directory. If the file is not in the right location under the project root
you'll get a warning message. This helps ensure relative paths are all working 
as expected. The chunk below uses the `SourceDir` and `SourceFile` parameters 
set in the YAML header. 

``` {r}
#| label: declare-path

# Declare path to this script relative to the project root directory.
here::i_am(path = paste0(params$SourceDir, params$SourceFile))
```

\FloatBarrier

## Load Data {#sec-Load-Data}
This subsection loads the data created by rendering `scripts/Import_Data.qmd`. 
The data is de-identified to preserve participant privacy and protect 
confidentiality. 

``` {r}
#| label: load-data
#| eval: true

# Store path to data file. 
DataFile <- here("data/Imported_SANETP_Data.RData")

load(file = DataFile)
```

@tbl-imported-data-file shows meta-data about the data file we just loaded and
@tbl-datasets shows the sizes of the datasets it contains. 

```{r}
#| label: tbl-imported-data-file
#| tbl-cap: "Meta-Data About the Data File Loaded"

file_details(DataFile) %>% 
  kable(, format = "latex", booktabs = TRUE, 
        col.names = c("File Name", "Size", "Last Modified")) %>% 
  kable_styling() 
```

```{r}
#| label: tbl-datasets
#| tbl-cap: "Sizes of the Datasets"

# Compute sample sizes and percent omitted by listwise deletion
N_All  <- nrow(Enrolled_Applicants)
N_CD   <- nrow(Enrolled_Applicants_CD)
N_LWD  <- N_All - N_CD
P_LWD  <- 100*N_LWD/N_All

data.frame(Dataset = c("Applicants", "Eligible_Applicants", 
                       "Enrolled_Applicants", "Enrolled_Applicants_CD", 
                       "Thresholds"),
           N_Rows = c(nrow(Applicants), nrow(Eligible_Applicants), 
                      nrow(Enrolled_Applicants), nrow(Enrolled_Applicants_CD), 
                      nrow(Thresholds)),
           N_Cols = c(ncol(Applicants), ncol(Eligible_Applicants),
                      ncol(Enrolled_Applicants), ncol(Enrolled_Applicants_CD), 
                      ncol(Thresholds))) %>% 
  kable(, format = "latex", booktabs = TRUE, 
        col.names = c("Dataset", "N Rows", "N Columns")) %>% 
  kable_styling() 
```

\FloatBarrier

# Overview of Stopping-Ratio Modeling {#sec-SR-Modeling}
Attrition from the training program can be conceptualized as a sequential 
filtering process. There are various threshold points between stages of program 
participation where a participant may either attrit or continue participating.
Each of those thresholds is a filter: participants that attrit at a given 
threshold are filtered out of the program. That reduces the number of
participants who reach the next stage of program participation and encounter the
next threshold.

That means an eligible applicant's progress through the training program can be
measured by an ordinal stage variable that records the maximum stage reached by
the applicant in that sequential filtering process. In this study, the ordinal 
variable is called `Stage_Reached`. It is a multinomial ordinal variable with 
$J = 4$ possible stages and observed values denoted by stage $j$, where 
$j \in \{1, 2, 3, 4\}$. 

@fig-Stages shows the final set of stages and thresholds (T1 to T3) between them
at which attrition from the training program could occur. The arrows associated
with each threshold are labeled according to how the outcome variable is coded
on the corresponding person-threshold record, depending on whether the person
stopped participating at the current stage or moved on to the next stage. 

```{dot}
//| label: fig-Stages
//| fig-cap: Stages, Thresholds (T1-T3), and Stopping Ratios (SR1-SR3) in the 
//|          SANE Training Program. CSW, clinical skills workshop; DT, didactic 
//|          training.
//| fig-width: 5
//| fig-height: 2.5

digraph StagesModeled {

graph [rankdir="LR"];

node [shape = "box", style= "filled", fillcolor = "Gray90", fontsize = "7"];
A1 [label = "Attrited\nBefore DT\nSR1"]
A2 [label = "Attrited\nDuring DT\nSR2"]
A3 [label = "Attrited\nBefore/During CSW\nSR3"]

S1 [label = "Stage 1\nEnrolled\nj = 1"]
S2 [label = "Stage 2\nStarted DT\nj = 2"]
S3 [label = "Stage 3\nFinished DT\nj = 3"]
S4 [label = "Stage 4\nFinished CSW\nj = 4"]

edge [fontsize = "7", arrowsize = 0.5];

S1 -> A1 [label = "T1\nAttrit = 1"]
S2 -> A2 [label = "T2\nAttrit = 1"]
S3 -> A3 [label = "T3\nAttrit = 1"]

S1 -> S2 [label = "Attrit = 0"]
S2 -> S3 [label = "Attrit = 0"]
S3 -> S4 [label = "Attrit = 0"]
}
```

There are several variations of regression models designed to analyze ordinal
outcomes: they test different hypotheses, estimate different parameters, and
yield different insights [@Fullerton-RN8774]. The research questions for this
study are best aligned with a broad class of ordinal regression models usually
called continuation-ratio (CR) models, though they are also called 
stopping-ratio (SR) models or stage models 
[@Fullerton-RN8774; @Liu-RN8772; @Smithson-RN2775; @Yee-RN3711]. The CR model is
ideally suited for ordinal outcome data generated from a sequential selection
process where all individuals start from the same initial stage, must pass
through earlier stages to reach later ones, and all stage transitions are
irreversible [@Fullerton-RN8774; @OConnell-RN8769]. Because our focus is on
predicting attrition--which stops a person's progression to the next stage of
program participation--we will use the SR nomenclature in most of this document
except when referring to broader literature that uses the CR model nomenclature.
It should be understood that CR models and SR models are the same statistical
method. It mostly doesn't matter which which term you use.

@Liu-RN8772 distinguished between forward and backward types of CR models and 
emphasized that each type can be expressed in two different sub-models depending 
on which of the two complementary conditional probabilities for a binary event 
one wants to emphasize (odds versus inversed odds). Regression coefficients 
from the two sub-models of the same type of CR model (forward or backward) have 
the same magnitude but opposite signs. The sub-models for a given type (e.g., 
forward) will also have the same model fit. However, results and interpretation 
will differ between types: the forward and backward CR models are not equivalent 
because they test different hypotheses. This study uses what @Liu-RN8772 called 
a forward CR model of sub-type A. That means we are trying to estimate the 
conditional probability of stopping at stage $j$ conditional on being in or 
above that category given a set of predictors. 

One key to understanding SR models is that they divide the analysis of a single
ordinal outcome into a series of binary outcomes regarding what happens at each
of the thresholds between stages. To run the planned model, we first reorganize
the data from one row per person to one row per person per threshold attempted
[@Cole-RN8770; @Fullerton-RN8774]. That reorganization allows us to use standard
logistic regression modeling software to fit the model and flexibly choose
whether predictor effects are parallel (constrained to equality across
thresholds), or non-parallel (unconstrained and allowed to vary across
thresholds) by either omitting or including interaction terms. Because the 
parallel effect model is nested within the non-parallel effect model, we can 
use likelihood ratio tests to discern whether allowing non-parallel effects 
improves model fit [@Cole-RN8770]. 

The SR model is attractive because its parameters can be translated into 
additional estimates and graphs that are easy to interpret and meaningful for 
assessing the effects of predictors on program attrition. One prior study 
compared CR models to other available models (e.g., logistic regression and 
cumulative odds models for ordinal outcomes) for examining student persistence 
through the remedial math sequence and successfully passing a college-level 
credit bearing math course at 2- and 4-year public institutions 
[@Davidson-RN8773]. @Davidson-RN8773 concluded that CR models provided richer 
insights, in part because they revealed non-parallel effects of some predictors 
that could not be discerned with the other kinds of models. Other studies have 
used CR models to examine predictors of student proficiency in math 
[@Liu-RN8771], to validate student self-assessments of oral language proficiency 
in second language learning [@Winke-RN3795], and examine predictors of how far 
sexual assault kits progressed through several stages of forensic testing 
[@Campbell-RN3126; @Campbell-RN3149; @Campbell-RN3190].

\FloatBarrier

# Methods

\FloatBarrier

## Sample
The sample consists of the enrolled applicants to the program who had complete 
data on all the measures listed in the next section. The 
`Enrolled_Applicants_CD` dataset contains person-level data ($N = `r N_CD`$) 
about that sample. Those data were rearranged into the `Thresholds` dataset 
($N = `r nrow(Thresholds)`$) before fitting the stopping-ratio models (see the
data import output for documentation of how that was done and
@sec-SR-Modeling for why it was necessary). 

We aim to draw conclusions that generalize to the overall population of enrolled
applicants to the program. We are using listwise deletion to handle missing data
because only `r N_LWD` of `r N_All` (`r round(P_LWD, digits = 1)`%) enrolled 
applicants had incomplete data (see the output from the data import script). 
These cases would have to be very influential for omitting them to induce 
substantial bias in the model results. When more than 5% of cases have missing
data, more sophisticated approaches may be required [@Fernández-García-RN4151],
but this dataset is far below that threshold.

When answering the part of RQ2 pertaining to specifice training modules, we need 
to narrow our focus to the subset of `Enrolled_Applicants_CD` containing only 
enrolled applicants with `Stage_Reached` $\ge 2$ (those who started the didactic 
training). Thus, below we create that additional dataset and call it 
`StartedDT_Applicants`. 

```{r}
#| label: create-StartedDT-Applicants

StartedDT_Applicants <- Enrolled_Applicants_CD %>% 
  filter(Stage_Reached >= 2)
```

\FloatBarrier

## Measures
The measures mentioned below include an outcome variable, a structural predictor
required for SR modeling, and eight focal predictors.

\FloatBarrier

### Attrition (Outcome Variable)
This is the binary, threshold-specific transformation of the `Stage_Reached` 
ordinal outcome variable. It is the outcome modeled in our SR models. The 
variable `Attrit` is coded 1 when an applicant attrited, and 0 when they passed 
the threshold and continued to the next stage of the program. See @fig-Stages 
for a visualization of the coding for `Attrit`. 

\FloatBarrier

### Threshold (Structural Predictor)
The `Threshold` variable identifies which of three thresholds a participant was 
attempting on any given record in the `Threshods` dataset. See @fig-Stages for 
an overview of the stages and thresholds. 

\FloatBarrier

### Background Characteristics (Focal Predictor)
The primary setting where the trainee practices nursing was a nominal
categorical factor named `Setting`. It originally had four levels, but was
recoded to three levels (`Urban`, `Rural/Tribal`, and `Suburban`) due to small
sample size in the `Tribal` category (which was then combined with `Rural`). See
the descriptive analyses output file for this study for more details. The
reference level is `Urban`.

\FloatBarrier

### Motivations (Focal Predictors)
The two motivation measures are binary (coded 0 = *No*, 1 = *Yes*) and include 
whether the applicant was motivated to seek the training by (a) a need for SANE 
services in their community or organization (`Motivation_NeedSANE`), and (b) a 
personal connection to sexual assault (e.g., someone they know is a survivor)
(`Motivation_PersonalConn`). The reference level for each of them is *No*. 

\FloatBarrier

### Potential Barriers (Focal Predictors)
The potential barriers outcome variables are single-item measures of barriers to
participation due to family obligations (`Barrier_FO`) and work responsibilities
(`Barrier_WR`) as competing demands on applicants' time. While these are
technically ordinal items following a 5-point Likert-response format, that
should be enough categories to treat them as continuous variables for these
analyses. Both measures were mean-centered before use in the model. See the 
descriptive analyses output file for this study for more details. 

\FloatBarrier

### Emotional Readiness (Focal Predictors)
The emotional readiness measures are all continuous scores measured by ProQOL 
subscales for burnout (`ProQOL_BO`), compassion satisfaction (`ProQOL_CS`), and 
secondary traumatic stress (`ProQOL_STS`) [@Stamm-RN8775]. They were all 
mean-centered prior to use in the model. 

\FloatBarrier

## Planned Statistical Analyses {#sec-Planned-Analyses}
Below we fit and examine a series of SR models by using the `glm()` function to 
run logistic regression models on a person-threshold dataset called 
`Thresholds`. This is one way to fit SR models [@Cole-RN8770]. 

The modeling strategy aims to estimate the minimum number of models required to
answer research questions RQ2 and RQ3 and obtain a parsimonious final model. All
models use the structural thresholds variable to predict attrition because that
lays the foundation for a SR model. 

### RQ2: What are the attrition rates from enrollment to completion of the CSW, and when (i.e., at which module[s]) did participants commonly attrit?

The simplest way to answer the first part of RQ2 is to fit a model that only
accounts for the threshold effect, then extract the estimated marginal means 
(EMMs) representing the attrition rate (i.e., the stopping-ratio) on the
probability scale at each threshold. Thus, our modeling sequence starts by
fitting Model 1, which only uses the structural thresholds variable as a
predictor. However, we can get a more nuanced answer to RQ2 by examining EMMs
extracted from a model that adjusts for covariates. We already plan to fit such
models to test the effects of focal predictors relevant to RQ3. Therefore it may
make sense to use EMMs from the model that provides the best answer to RQ3
rather than the EMMs from Model 1.

We used the a subset of the `StartedDT_Applicants_CD` dataset to examine the 
proportions of applicants who started and finished each DT module by estimating 
the means of relevant binary variables (`Started_Mod_*` and `Finished_Mod_*`) to 
obtain the start and finish rates for modules 1-12. We used Wilson score
confidence intervals to quantify the uncertainty in those rates
[@Newcombe-RN2457; @Wilson-RN3223]. 

::: {.callout-caution}
Missing data for the `Started_Mod_*` and `Finished_Mod_*` variables are 
currently treated as functionally equivalent to 0 (*not started* or 
*not finished*) in the rate calculations. So, the denominator includes cases 
with missing values. We can change that if desired. 
:::

### RQ3: Do participants’ background characteristics, motivations for seeking training, potential barriers, and emotional readiness for this work predict attrition rates? 

Answering RQ3 properly requires determining whether the focal predictors have 
parallel or non-parallel effects on attrition. Model 1 is the baseline model we
can compare with models containing the focal predictors mentioned in RQ3. Model
2 adds the set of parallel effects for all eight focal predictors, then Model 3
expands on Model 2 by adding interaction terms to estimate non-parallel effects
for all eight focal predictors [@Cole-RN8770].

::: {.callout-tip}
After testing for non-parallel effects, we have to decide whether to simplify 
the better model (either Model 2 or Model 3) by omitting focal predictors that 
do not seem to have any effect at all. Doing so could lead to a more 
parsimonious model, but sacrifices the ability to claim that our results for 
retained predictors reflect having controlled for all the omitted focal 
predictors. Only reporting Models 1-3 would reduce the total amount of output 
in the manuscript. *That is minimum set of models required to address RQ3.* 
::: 

As a robustness check, we tried simplifying the models. Models 4 and 5 aim to
achieve parsimony by only including the one focal predictor that demonstrated an
effect in Model 2. Model 4 builds up from Model 1 by adding a parallel effect
for that focal predictor, then Model 5 further extends the analysis to test
whether it had a non-parallel effect.

We examined model diagnostics and fit statistics, model comparisons via 
likelihood ratio tests, single-term deletion tests based on Type II sums of 
squares, and other supplementary output. Effect sizes are quantified by 
odds-ratios and EMMs for the attrition rates reported on a probability scale. 

## Modeling Software
We used the base R [@R-Devel-Core-RN8182] function `glm()` to fit the models
because it is a high-quality, reliable, and flexible tool for fitting logistic 
regression models. We also used several additional R packages. The `broom` 
package [@Robinson-RN8669] facilitated extracting model parameters, odds-ratios, 
and associated confidence intervals, while the `performance` package 
[@Lüdecke-RN8788] extracted model fit statistics. The `DHARMa` [@Hartig-RN8668] 
and `car` [@Fox-RN8789] packages provided tools for examining model diagnostics
(see next section) and the `emmeans` package [@Lenth-RN8188] computed estimated
marginal means that represent attrition rates.

\FloatBarrier

## Model Diagnostics

\FloatBarrier

### Collinearity
The `car` package [@Fox-RN8789]  provides a function for examining the
generalized variance inflation factor (GVIF) for terms in regression models.
We checked for multicollinearity problems by examining the GVIF and a 
transformation of it that adjusts for categorical terms that use multiple 
degrees of freedom [@Fox-RN3484]. In general, one can compare 
$GVIF^{(1/(2*df))}$ to cutoffs defined for $\sqrt{VIF}$. Some suggested cutoffs 
indicating potential problems are $\sqrt{VIF} \ge 2$ or $\sqrt{VIF} \ge 3$. We 
applied the latter cutoff, so hope to see $GVIF^{(1/(2*df))} < 3$ in our models.

\FloatBarrier

### Residual Distributions 
The `DHARMa` package in R uses simulation-based methods to compute scaled
residuals that should follow a uniform distribution if the data are consistent
with the actual distribution used to model the outcome, making it easier to
check for deviations from model assumptions [@Hartig-RN8668]. We used those 
residuals to run several tests of model assumptions. 

First, we conducted a Kolmogorov-Smirnov (K-S) test of overall uniformity of the 
residual distribution, pairing it with a quantile-quantile (Q-Q) plot of the 
observed versus expected residual values. As usual with a Q-Q plot, data points 
should lay close to the reference line for the theoretical distribution. 

Second, we split the data by threshold and used both a K-S test of within group
deviation from uniformity and a Levene test for homogeneity of variance, pairing
the result with a a set of box plots showing the residual distribution within a
specific level of threshold. That plot should show the upper and lower edges of
the boxes laying close to the dashed lines for the 25th and 75th quantiles, and
the median bar of the box laying at the 50th quantile. Heterogeneity of variance
would be evident if the boxes associated with different thresholds vary
substantially in height.

Third, we conducted two-sided binomial tests for outliers on either end of the
residual distribution. The accompanying plot is a histogram of the residual 
distribution, annotated with red lines for any outliers detected. 

Finally, we ran a simulation-based a test for over- or under-dispersion in the
the scaled residuals. This is paired with a histogram of the distribution of 
simulated dispersion values, with a red line marking the position of the 
observed dispersion parameter value. 

We applied all of these `DHARMa` tests to the models below, summarizing the
results in figures. For all of the `DHARMa` tests, good models should produce
large $p$-values (sometimes displayed on plots as just `n.s.`) because small
values (e.g., $p < 0.05$) indicate violation of model assumptions.

# Results

\FloatBarrier

## Model 1: Thresholds Alone
The modeling starts by first fitting a very simple model (`m1`) containing only 
a threshold main effect. This should allow is to see whether attrition rate 
varies across the thresholds. @tbl-params-m1 shows the model parameters and 
@tbl-fit-m1 shows its fit statistics. 

```{r}
#| label: tbl-params-m1
#| tbl-cap: "Model 1 Parameters"

m1 <- glm(Attrit ~ Threshold. + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m1) %>% 
  cbind(confint.default(m1)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2))
```

```{r}
#| label: tbl-fit-m1
#| tbl-cap: "Model 1 Fit Statistics"

glance(m1) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Single Term Deletion Tests
@tbl-drop1-m1 shows the effect of deleting the threshold term from the model to 
determine whether doing so worsens model fit. A significant effect should be 
retained in the model because omitting it would damage the model fit.

```{r}
#| label: tbl-drop1-m1
#| tbl-cap: "Model 1 Single Term Deletion Tests (Type II SS)"
#| warning: false

m1 %>% 
  drop1(., test = "LRT") %>% 
  tidy() %>% 
  rename(p = p.value) %>% 
  mutate(p = display_num(p)) %>% 
  kable(format = "latex", booktabs = TRUE,
        digits = c(0, 0, 2, 2, 2, Inf),
        col.names = c("Term Deleted", "df", "Deviance", "AIC", "LRT", "p")) %>% 
  kable_styling() 
```

::: {.callout-tip}
The threshold main effect is integral to fitting a stopping-ratio model. It must
remain in the model to properly test for both parallel and non-parallel effects
of focal predictors. Removing the threshold main effect would also harm model
fit, so we will keep it in all subsequent models.
:::

\FloatBarrier

### Estimated Marginal Means
The presence of a threshold main effect shows that the attrition rate varies
across thresholds. Next we compute estimated marginal means (EMMs) for each
threshold and back-transform them to the response (probability) scale to see the
attrition rate at each threshold. We expect these rates to agree with the simple
descriptive statistics for percent attrition at each threshold because threshold
is the sole predictor in Model 1. @tbl-emmeans-m1 and  @fig-emmeans-m1 show the 
EMMs derived from Model 1.

```{r}
#| label: tbl-emmeans-m1
#| tbl-cap: "Model 1 Conditional Attrition Rate by Threshold"

FN <- paste("Values are on the probability scale.",
            "EMM, estimated marginal mean.")

TLabels1 = c("T1 Before DT", 
             "T2 During DT",
             "T3 Before/During CSW")

emmeans(m1, specs = ~ Threshold., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  mutate(Threshold. = factor(Threshold., labels = TLabels1)) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Threshold", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m1
#| fig-cap: "Model 1 Conditional Attrition Rate by Threshold (Main Effect). 
#|           CSW, clinical skills workshop; DT, didactic training."
#| fig-width: 4
#| fig-height: 2.5

TLabels2 = c("T1 Before DT", 
            "T2 During DT",
            "T3 Before/\nDuring CSW")

emmeans(m1, specs = ~ Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Threshold") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01)) +
  scale_y_discrete(labels = TLabels2) + 
  theme_bw()
```

\FloatBarrier

### Model Diagnostics
@fig-m1-diagnose shows the plots for diagnostic tests of model assumptions. 

``` {r}
#| label: fig-m1-diagnose
#| fig-cap: "Diagnostic Plots for Model 1"
#| fig-subcap: 
#| - "Q-Q Plot and Test for Overall Uniformity"
#| - "Residual Boxplots by Threshold"
#| - "Binomial Outlier Test"
#| - "Nonparametric Dispersion Test"
#| fig.width: 6
#| fig.height: 3
#| layout-ncol: 2
#| layout-nrow: 2

# Save simulated residuals for use in diagnostics using DHARMa. 
# Seed uses a parameter defined in YAML header for reproducibility.
m1_sim <- simulateResiduals(fittedModel = m1, n = 1000, method = "PIT",
                                seed = params$DHARMa.seed1)

plotQQunif(simulationOutput = m1_sim, testUniformity = TRUE, 
           testDispersion = FALSE, testOutliers = FALSE)

invisible(testCategorical(simulationOutput = m1_sim, 
                          catPred = as_factor(Thresholds$Threshold), 
                          quantiles = c(0.25, 0.5, 0.75), plot = TRUE)) 

invisible(testOutliers(simulationOutput = m1_sim, alternative  = "two.sided", 
                       margin = "both", type = "binomial", plot = TRUE))

invisible(testDispersion(simulationOutput = m1_sim, alternative = "two.sided", 
                         plot = TRUE, type = "DHARMa"))
```

:::{.callout-tip}
Model 1 assumptions are plausible. The scaled residuals appear to be uniform
both overall (@fig-m1-diagnose-1) and within each threshold with no signs of
heterogeneity of variance (@fig-m1-diagnose-2). There are no outliers
(@fig-m1-diagnose-3), and there is no evidence of over- or under-dispersion
(@fig-m1-diagnose-4).
:::

\FloatBarrier

## Model 2: Thresholds & Parallel Effects for All Focal Predictors
In SR models, a predictor has a parallel effect when its regression coefficient 
is constrained equal across all thresholds. This is implemented by estimating 
only a main effect for the predictor (it cannot interact with the threshold). 
Model 2 (`m2`) uses parallel effects for the focal predictors. See 
@tbl-params-m2 for parameter estimates and @tbl-fit-m2 for the fit statistics. 

```{r}
#| label: tbl-params-m2
#| tbl-cap: "Model 2 Parameters"

m2 <- glm(Attrit ~ Threshold. + Setting. + Motivation_NeedSANE. + 
            Motivation_PersonalConn. + CBarrier_FO + CBarrier_WR + 
            CProQOL_BO + CProQOL_CS + CProQOL_STS  + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m2) %>% 
  cbind(confint.default(m2)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2))
```

```{r}
#| label: tbl-fit-m2
#| tbl-cap: "Model 2 Fit Statistics"

glance(m2) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Check for Multicollinearity
@tbl-GVIF-m2 shows the generalized variance inflation factors (GVIF) for Model 
2.

```{r}
#| label: tbl-GVIF-m2
#| tbl-cap: Model 2 Generalized Variance Inflation Factors for Predictors

FN <- paste("Values >= 3 in the last column indicate multicollinearity problems.")

car::vif(m2) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Term") %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

::: {.callout-tip}
We do not appear to have any multicollinearity problems in Model 2.
:::

\FloatBarrier

### Compare Models 1 and 2
@tbl-lrt-m1-m2 shows a likelihood ratio test (LRT) comparing Models 1 and 2. 

```{r}
#| label: tbl-lrt-m1-m2
#| tbl-cap: "Likelihood Ratio Test Comparing Models 1 and 2"

test_lrt(m1, m2) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m1", 
                         true = "Thesholds", 
                         false = "Thresholds + All Parallel Effects")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

\FloatBarrier

::: {.callout-caution}
This is a simultaneous test of whether adding parallel effects for all of the
focal predictors as a block improved the model fit enough to justify adding that
many extra parameters. It did not. However, the Wald tests in @tbl-params-m2 
suggest that `Motivation_NeedSANE` has an effect while none of the other
predictors do. We need to look more closely at the effects of individual
predictors via single-term deletion tests.
:::

\FloatBarrier

### Single Term Deletion Tests
@tbl-drop1-m2 shows the effect of deleting specific terms from the model to 
determine whether doing so worsens model fit. Each main effect term is tested 
after controlling for all the other main effects. This is a more focused test 
of individual predictors than the overall LRT comparing Models 1 and 2. 
Significant effects should be retained in the model because omitting them would 
damage the model fit. Non-significant effects can potentially be omitted. 

```{r}
#| label: tbl-drop1-m2
#| tbl-cap: "Model 2 Single Term Deletion Tests (Type II SS)"
#| warning: false

m2 %>% 
  drop1(., test = "LRT") %>% 
  tidy() %>% 
  rename(p = p.value) %>% 
  mutate(p = display_num(p)) %>% 
  kable(format = "latex", booktabs = TRUE,
        digits = c(0, 0, 2, 2, 2, Inf),
        col.names = c("Term Deleted", "df", "Deviance", "AIC", "LRT", "p")) %>% 
  kable_styling() 
```

\FloatBarrier

::: {.callout-tip}
Only `Threshold` and `Motivation_NeedSANE` have main effects that need to be 
retained. We could in theory remove all the other focal predictors. 
:::

\FloatBarrier

### Estimated Marginal Means
Next we compute estimated marginal means (EMMs) for each threshold and 
back-transform them to the response (probability) scale to see the attrition 
rate at each threshold. @tbl-emmeans-m2-Threshold and @fig-emmeans-m2-Threshold 
show the EMMs derived from Model 2 for each threshold, while 
@tbl-emmeans-m2-NeedSANE and @fig-emmeans-m2-NeedSANE show the EMMs for each 
level of `Motivation_NeedSANE`. @tbl-emmeans-m2-Both and @fig-emmeans-m2-Both 
show the combined result of the two main effects while averaging over the
results of all other predictors.

```{r}
#| label: tbl-emmeans-m2-Threshold
#| tbl-cap: Model 2 Conditional Attrition Rate by Threshold (Main Effect)

FN <- paste("Values are on the probability scale.", 
            "Results are averaged over the levels of all other categorical", 
            "predictors and assume values at the mean on continuous focal", 
            "predictors.",
            "EMM, estimated marginal mean.")

emmeans(m2, specs = ~ Threshold., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  mutate(Threshold. = factor(Threshold., labels = TLabels1)) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Threshold", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m2-Threshold
#| fig-cap: Model 2 Conditional Attrition Rate by Threshold (Main Effect 
#|          Averaged Over All Other Predictors). CSW, clinical skills workshop;   
#|          DT, didactic training.
#| fig-width: 4
#| fig-height: 2.5

emmeans(m2, specs = ~ Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Threshold") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01))  +
  scale_y_discrete(labels = TLabels2) + 
  theme_bw()
```

```{r}
#| label: tbl-emmeans-m2-NeedSANE
#| tbl-cap: Model 2 Conditional Attrition Rate by Motivation_NeedSANE (Main 
#|          Effect)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Values are on the probability scale.",
            "Results are averaged over the levels of all other categorical", 
            "predictors and assume values at the mean on continuous focal", 
            "predictors.",
            "EMM, estimated marginal mean.")

emmeans(m2, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Motivation_NeedSANE.", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m2-NeedSANE
#| fig-cap: Model 2 Conditional Attrition Rate by Motivation_NeedSANE (Parallel 
#|          Main Effect Averaged Over All Other Predictors)
#| fig-width: 2
#| fig-height: 2.5

emmeans(m2, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01)) + 
  theme_bw()
```

```{r}
#| label: tbl-emmeans-m2-Both
#| tbl-cap: Model 2 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Results are averaged over the levels of all other categorical", 
            "predictors and assume values at the mean on continuous focal", 
            "predictors.",
            "Values are on the probability scale.", 
            "CSW, clinical skills workshop; DT, didactic training;",
            "EMM, estimated marginal mean.")

emmeans(m2, specs = ~ Threshold. | Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  arrange(Threshold., Motivation_NeedSANE.) %>% 
  select(!df) %>% 
  mutate(Threshold. = factor(Threshold., labels = TLabels1)) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 0, 3, 3, 3, 3),
        col.names = c("Threshold", "Motivation_NeedSANE.", "Rate (EMM)", "SE", 
                      "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 4, "95% Wald CI" = 2)) %>% 
  collapse_rows(columns = 1:2, valign = "top", latex_hline = "major", 
                custom_latex_hline = 1, headers_to_remove = 0,
                row_group_label_position = "stack") %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m2-Both
#| fig-cap: Model 2 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects, Averaged Over All Other 
#|          Predictors). CSW, clinical skills workshop; DT, didactic training.
#| fig-width: 4.5
#| fig-height: 2.5

FLabels = c("1" = "Threshold T1\nBefore DT", 
            "2" = "Threshold T2\nDuring DT",
            "3" = "Threshold T3\nBefore/During CSW")

emmeans(m2, specs = ~ Motivation_NeedSANE. | Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  facet_grid(cols = vars(Threshold.), labeller = labeller(Threshold. = FLabels)) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01)) + 
  theme_bw()
```

\FloatBarrier

### Model Diagnostics
@fig-m2-diagnose shows the plots for diagnostic tests of model assumptions. 

``` {r}
#| label: fig-m2-diagnose
#| fig-cap: "Diagnostic Plots for Model 2"
#| fig-subcap: 
#| - "Q-Q Plot and Test for Overall Uniformity"
#| - "Residual Boxplots by Threshold"
#| - "Binomial Outlier Test"
#| - "Nonparametric Dispersion Test"
#| fig.width: 6
#| fig.height: 3
#| layout-ncol: 2
#| layout-nrow: 2

# Save simulated residuals for use in diagnostics using DHARMa. 
# Seed uses a parameter defined in YAML header for reproducibility.
m2_sim <- simulateResiduals(fittedModel = m2, n = 1000, method = "PIT",
                                seed = params$DHARMa.seed2)

plotQQunif(simulationOutput = m2_sim, testUniformity = TRUE, 
           testDispersion = FALSE, testOutliers = FALSE)

invisible(testCategorical(simulationOutput = m2_sim, 
                          catPred = as_factor(Thresholds$Threshold), 
                          quantiles = c(0.25, 0.5, 0.75), plot = TRUE)) 

invisible(testOutliers(simulationOutput = m2_sim, alternative  = "two.sided", 
                       margin = "both", type = "binomial", plot = TRUE))

invisible(testDispersion(simulationOutput = m2_sim, alternative = "two.sided", 
                         plot = TRUE, type = "DHARMa"))
```

\FloatBarrier

:::{.callout-tip}
Model 2 assumptions are plausible. The scaled residuals appear to be uniform
both overall (@fig-m2-diagnose-1) and within each threshold with no signs of
heterogeneity of variance (@fig-m2-diagnose-2). There are no outliers
(@fig-m2-diagnose-3), and there is no evidence of over- or under-dispersion
(@fig-m2-diagnose-4).
:::

\FloatBarrier

## Model 3: Thresholds & Non-Parallel Effects for All Focal Predictors
In SR models, a predictor has a non-parallel effect when its regression 
coefficient is allowed to vary across all thresholds. That is implemented by 
allowing the predictor to have both a main effect and an interaction with the 
threshold. Continuous predictors must be appropriately centered before model 
fitting as usual when testing interactions (i.e., moderator hypotheses). Model 3 
(`m3`) treats threshold as a moderator of the other predictors' effects. See 
@tbl-params-m3 for parameter estimates and @tbl-fit-m3 for fit statistics. 

We are continuing to include predictors that did not have significant main 
effects in Model 2 because it is possible that threshold moderates the effects 
of other predictors such that there are non-parallel (threshold-specific) 
effects that get canceled out when averaging across them to estimate a parallel 
main effect. Thus, our approach maximizes the chance that we detect any 
unusually nuanced non-parallel effects if they exist. 

::: {.callout-caution}
Model 3 is rather ambitious given the sample size we have available. It is
adding eight interaction effects: that yields many extra parameters. This poses
some risk for over-fitting the model, but testing for non-parallel effects
either requires fitting this one model, or fitting a set of eight other models
where we add only a single non-parallel effect at a time. Fitting this larger
model is more efficient and reduces the total amount of output we have to
review.
:::

```{r}
#| label: tbl-params-m3
#| tbl-cap: "Model 3 Parameters"

m3 <- glm(Attrit ~ Threshold. + Setting. + Motivation_NeedSANE. + 
            Motivation_PersonalConn. + CBarrier_FO + CBarrier_WR + 
            CProQOL_BO + CProQOL_CS + CProQOL_STS  + 
            Threshold.:Setting. + 
            Threshold.:Motivation_NeedSANE. + 
            Threshold.:Motivation_PersonalConn. + 
            Threshold.:CBarrier_FO + Threshold.:CBarrier_WR + 
            Threshold.:CProQOL_BO + Threshold.:CProQOL_CS + 
            Threshold.:CProQOL_STS + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m3) %>% 
  cbind(confint.default(m3)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(Parameter = str_replace(string = Parameter, pattern = ":", 
                                 replacement = " x "), 
         p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling(font_size = 10) %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2)) %>% 
  column_spec(column = 1, width = "5.5cm")
```

```{r}
#| label: tbl-fit-m3
#| tbl-cap: "Model 3 Fit Statistics"

glance(m3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Check for Multicollinearity
@tbl-GVIF-m3 shows the generalized variance inflation factors (GVIF) for Model 
3. 

```{r}
#| label: tbl-GVIF-m3
#| tbl-cap: Model 3 Generalized Variance Inflation Factors for Predictors
#| warning: false

FN <- paste("Values >= 3 in the last column indicate multicollinearity problems.")

car::vif(m3) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Term") %>% 
  mutate(Term = str_replace(string = Term, pattern = ":", 
                            replacement = " x ")) %>%  
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

::: {.callout-tip}
We do not appear to have any multicollinearity problems in Model 3
:::

\FloatBarrier

### Compare Models 2 and 3
@tbl-lrt-m2-m3 shows a likelihood ratio test (LRT) comparing Models 2 and 3. 

```{r}
#| label: tbl-lrt-m2-m3
#| tbl-cap: "Likelihood Ratio Test Comparing Models 2 and 3"

test_lrt(m2, m3) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m2", 
                         true = "Thesholds + All Parallel Effects", 
                         false = "Thresholds + All Non-Parallel Effects")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

::: {.callout-caution}
This is a simultaneous test of whether adding non-parallel effects for all of 
the focal predictors as a block improved the model fit enough to justify adding 
that many extra parameters. Apparently it did not. We should still look more 
closely at the effects of individual predictors via single-term deletion tests.
:::

\FloatBarrier

### Single Term Deletion Tests
@tbl-drop1-m3 shows the effect of deleting specific interaction terms from the 
model to determine whether doing so worsens model fit. Significant effects 
should be retained in the model because omitting them would damage the model 
fit. We do not test main effects for predictors involved in interactions because 
we need to respect the principle of marginality 
[@Fox-RN3484; @Hector-RN3472; @Langsrud-RN1287]. 

```{r}
#| label: tbl-drop1-m3
#| tbl-cap: "Model 3 Single Term Deletion Tests (Type II SS)"
#| warning: false

m3 %>% 
  drop1(., test = "LRT") %>% 
  tidy() %>% 
  rename(p = p.value) %>% 
  mutate(term = str_replace(string = term, pattern = ":", 
                            replacement = " x "), 
         p = display_num(p)) %>% 
  kable(format = "latex", booktabs = TRUE,
        digits = c(0, 0, 2, 2, 2, Inf),
        col.names = c("Term Deleted", "df", "Deviance", "AIC", "LRT", "p")) %>% 
  kable_styling() 
```

\FloatBarrier

::: {.callout-tip}
We can safely drop all of the interaction effects from the model. None of them 
improve the model fit. It may be worth trying a final pair of models that are 
more parsimonious than Models 2 and 3: they should include models with threshold 
effect and just parallel and non-parallel effects of `Motivation_NeedSANE` 
without other focal predictors. 
:::

\FloatBarrier

### Model Diagnostics
@fig-m3-diagnose shows the plots for diagnostic tests of model assumptions. 

``` {r}
#| label: fig-m3-diagnose
#| fig-cap: "Diagnostic Plots for Model 3"
#| fig-subcap: 
#| - "Q-Q Plot and Test for Overall Uniformity"
#| - "Residual Boxplots by Threshold"
#| - "Binomial Outlier Test"
#| - "Nonparametric Dispersion Test"
#| fig.width: 6
#| fig.height: 3
#| layout-ncol: 2
#| layout-nrow: 2

# Save simulated residuals for use in diagnostics using DHARMa. 
# Seed uses a parameter defined in YAML header for reproducibility.
m3_sim <- simulateResiduals(fittedModel = m3, n = 1000, method = "PIT",
                                seed = params$DHARMa.seed3)

plotQQunif(simulationOutput = m3_sim, testUniformity = TRUE, 
           testDispersion = FALSE, testOutliers = FALSE)

invisible(testCategorical(simulationOutput = m3_sim, 
                          catPred = as_factor(Thresholds$Threshold), 
                          quantiles = c(0.25, 0.5, 0.75), plot = TRUE)) 

invisible(testOutliers(simulationOutput = m3_sim, alternative  = "two.sided", 
                       margin = "both", type = "binomial", plot = TRUE))

invisible(testDispersion(simulationOutput = m3_sim, alternative = "two.sided", 
                         plot = TRUE, type = "DHARMa"))
```

\FloatBarrier

:::{.callout-tip}
Model 3 assumptions are plausible. The scaled residuals appear to be uniform
both overall (@fig-m3-diagnose-1) and within each threshold with no signs of
heterogeneity of variance (@fig-m3-diagnose-2). There are no outliers
(@fig-m3-diagnose-3), and there is no evidence of over- or under-dispersion
(@fig-m3-diagnose-4).
:::

\FloatBarrier

## Model 4: Thresholds & Parallel Effect for `Motivation_NeedSANE`
Model 4 uses a parallel effect for `Motivation_NeedSANE` without any of the 
other focal predictors. @tbl-params-m4 shows the parameter estimates and 
@tbl-fit-m4 shows the fit indices. 

```{r}
#| label: tbl-params-m4
#| tbl-cap: "Model 4 Parameters"

m4 <- glm(Attrit ~ Threshold. + Motivation_NeedSANE. + 1 , 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m4) %>% 
  cbind(confint.default(m4)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2))
```

```{r}
#| label: tbl-fit-m4
#| tbl-cap: "Model 4 Fit Statistics"

glance(m4) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Check for Multicollinearity
Model 4 has only a subset of the predictors that were in Model 3, which had no 
multicollinearity issues. Therefore we skip checking for it here. 

\FloatBarrier

### Compare Models 1 and 4
@tbl-lrt-m1-m2 shows a likelihood ratio test (LRT) comparing Models 1 and 4. 

```{r}
#| label: tbl-lrt-m1-m4
#| tbl-cap: "Likelihood Ratio Test Comparing Models 1 and 4"

test_lrt(m1, m4) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m1", 
                         true = "Thesholds", 
                         false = "Thresholds + Parallel Motivation_NeedSANE Effect")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

::: {.callout-caution}
While we saw an effect of `Motivation_NeedSANE` in Model 2 when controlling for 
all the other focal predictors, we no longer see one in Model 4 when it is the 
sole focal predictor. This is evident in both @tbl-params-m4 and @tbl-lrt-m1-m4.
That suggests controlling for other focal predictors is important in revealing
the effect. Comparing the coefficients and odds-ratios for that predictor
between @tbl-params-m2 and @tbl-params-m4 shows that sign of the effect is
consistent across models but it is stronger and has a narrower confidence
interval for the odds-ratio in Model 2 than in Model 4. I think this may be 
because one or more of the other focal predictors is a confounder or suppressor
variable. I'm inclined to rely more on Model 2 than Model 4. 
:::

\FloatBarrier

### Estimated Marginal Means
Next we compute estimated marginal means (EMMs) for each threshold and 
back-transform them to the response (probability) scale to see the attrition 
rate at each threshold. @tbl-emmeans-m4-Threshold and @fig-emmeans-m4-Threshold 
show the EMMs derived from Model 4 for each threshold, while 
@tbl-emmeans-m4-NeedSANE and @fig-emmeans-m4-NeedSANE show the EMMs for each 
level of `Motivation_NeedSANE`. @tbl-emmeans-m4-Both and @fig-emmeans-m4-Both 
show the combined result of the two main effects .

```{r}
#| label: tbl-emmeans-m4-Threshold
#| tbl-cap: Model 4 Conditional Attrition Rate by Threshold (Main Effect)

FN <- paste("Values are on the probability scale.", 
            "Results are averaged over the levels of Motivation_NeedSANE.",
            "EMM, estimated marginal mean.")

emmeans(m4, specs = ~ Threshold., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  mutate(Threshold. = factor(Threshold., labels = TLabels1)) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Threshold", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m4-Threshold
#| fig-cap: Model 4 Conditional Attrition Rate by Threshold (Main Effect 
#|          Averaged Over Motivation_NeedSANE). CSW, clinical skills workshop;   
#|          DT, didactic training.
#| fig-width: 4
#| fig-height: 2.5

emmeans(m4, specs = ~ Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Threshold") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01)) +
  scale_y_discrete(labels = TLabels2) + 
  theme_bw()
```

```{r}
#| label: tbl-emmeans-m4-NeedSANE
#| tbl-cap: Model 4 Conditional Attrition Rate by Motivation_NeedSANE (Main 
#|          Effect)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Values are on the probability scale.",
            "Results are averaged over the levels of Threshold.",
            "EMM, estimated marginal mean.")

emmeans(m4, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Motivation_NeedSANE.", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m4-NeedSANE
#| fig-cap: Model 4 Conditional Attrition Rate by Motivation_NeedSANE (Parallel 
#|          Main Effect Averaged Over Threshold)
#| fig-width: 2
#| fig-height: 2.5

emmeans(m4, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01)) +
  theme_bw()
```

```{r}
#| label: tbl-emmeans-m4-Both
#| tbl-cap: Model 4 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Values are on the probability scale.", 
            "CSW, clinical skills workshop; DT, didactic training;",
            "EMM, estimated marginal mean.")

emmeans(m4, specs = ~ Threshold. | Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  arrange(Threshold., Motivation_NeedSANE.) %>% 
  select(!df) %>% 
  mutate(Threshold. = factor(Threshold., labels = TLabels1)) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 0, 3, 3, 3, 3),
        col.names = c("Threshold", "Motivation_NeedSANE.", "Rate (EMM)", "SE", 
                      "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 4, "95% Wald CI" = 2)) %>% 
  collapse_rows(columns = 1:2, valign = "top", latex_hline = "major", 
                custom_latex_hline = 1, headers_to_remove = 0,
                row_group_label_position = "stack") %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m4-Both
#| fig-cap: Model 4 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects Combined). CSW, clinical skills 
#|          workshop; DT, didactic training.
#| fig-width: 4.5
#| fig-height: 2.5

emmeans(m4, specs = ~ Motivation_NeedSANE. | Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  facet_grid(cols = vars(Threshold.), labeller = labeller(Threshold. = FLabels)) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), 
                     labels = label_number(accuracy = 0.01)) +
  theme_bw()
```

\FloatBarrier

### Model Diagnostics
@fig-m4-diagnose shows the plots for diagnostic tests of model assumptions. 

``` {r}
#| label: fig-m4-diagnose
#| fig-cap: "Diagnostic Plots for Model 4"
#| fig-subcap: 
#| - "Q-Q Plot and Test for Overall Uniformity"
#| - "Residual Boxplots by Threshold"
#| - "Binomial Outlier Test"
#| - "Nonparametric Dispersion Test"
#| fig.width: 6
#| fig.height: 3
#| layout-ncol: 2
#| layout-nrow: 2

# Save simulated residuals for use in diagnostics using DHARMa. 
# Seed uses a parameter defined in YAML header for reproducibility.
m4_sim <- simulateResiduals(fittedModel = m4, n = 1000, method = "PIT",
                                seed = params$DHARMa.seed4)

plotQQunif(simulationOutput = m4_sim, testUniformity = TRUE, 
           testDispersion = FALSE, testOutliers = FALSE)

invisible(testCategorical(simulationOutput = m4_sim, 
                          catPred = as_factor(Thresholds$Threshold), 
                          quantiles = c(0.25, 0.5, 0.75), plot = TRUE)) 

invisible(testOutliers(simulationOutput = m4_sim, alternative  = "two.sided", 
                       margin = "both", type = "binomial", plot = TRUE))

invisible(testDispersion(simulationOutput = m4_sim, alternative = "two.sided", 
                         plot = TRUE, type = "DHARMa"))
```

:::{.callout-tip}
Model 4 assumptions are plausible. The scaled residuals appear to be uniform
both overall (@fig-m4-diagnose-1) and within each threshold with no signs of
heterogeneity of variance (@fig-m4-diagnose-2). There are no outliers
(@fig-m4-diagnose-3), and there is no evidence of over- or under-dispersion
(@fig-m4-diagnose-4).
:::

\FloatBarrier

## Model 5: Thresholds & Non-Parallel Effect for `Motivation_NeedSANE` 
Model 5 uses a non-parallel effect for `Motivation_NeedSANE` without any of the 
other focal predictors. @tbl-params-m5 shows the parameter estimates and 
@tbl-fit-m5 shows the fit indices. 

```{r}
#| label: tbl-params-m5
#| tbl-cap: "Model 5 Parameters"

m5 <- glm(Attrit ~ Threshold. + Motivation_NeedSANE. + 
          Threshold.:Motivation_NeedSANE. + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m5) %>% 
  cbind(confint.default(m5)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(Parameter = str_replace(string = Parameter, pattern = ":", 
                                 replacement = " x "), 
         p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling(font_size = 10) %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2)) %>% 
  column_spec(column = 1, width = "5.5cm")
```

```{r}
#| label: tbl-fit-m5
#| tbl-cap: "Model 5 Fit Statistics"

glance(m5) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Check for Multicollinearity
Model 5 has only a subset of the predictors that were in Model 3, which had no 
multicollinearity issues. Therefore we skip checking for it here. 

\FloatBarrier

### Compare Models 4 and 5
@tbl-lrt-m4-m5 shows a likelihood ratio test (LRT) comparing Models 4 and 5. 

```{r}
#| label: tbl-lrt-m4-m5
#| tbl-cap: "Likelihood Ratio Test Comparing Models 4 and 5"

test_lrt(m4, m5) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m4", 
                         true = "Thesholds + Parallel Effect of Motivation_NeedSANE", 
                         false = "Thresholds + Non-Parallel Effect of Motivation_NeedSANE")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

::: {.callout-tip}
This tests whether adding a non-parallel effect for `Motivation_NeedSANE` 
improved the model fit enough to justify adding that many extra parameters. It 
did not. We can safely drop the interaction effect from the model. That tells us
we that if we're comfortable pursuing a more parsimonious model than Model 2, we
should interpret Model 4 instead of Model 5.
:::

\FloatBarrier

### Model Diagnostics
@fig-m5-diagnose shows the plots for diagnostic tests of model assumptions. 

``` {r}
#| label: fig-m5-diagnose
#| fig-cap: "Diagnostic Plots for Model 5"
#| fig-subcap: 
#| - "Q-Q Plot and Test for Overall Uniformity"
#| - "Residual Boxplots by Threshold"
#| - "Binomial Outlier Test"
#| - "Nonparametric Dispersion Test"
#| fig.width: 6
#| fig.height: 3
#| layout-ncol: 2
#| layout-nrow: 2

# Save simulated residuals for use in diagnostics using DHARMa. 
# Seed uses a parameter defined in YAML header for reproducibility.
m5_sim <- simulateResiduals(fittedModel = m5, n = 1000, method = "PIT",
                                seed = params$DHARMa.seed5)

plotQQunif(simulationOutput = m5_sim, testUniformity = TRUE, 
           testDispersion = FALSE, testOutliers = FALSE)

invisible(testCategorical(simulationOutput = m5_sim, 
                          catPred = as_factor(Thresholds$Threshold), 
                          quantiles = c(0.25, 0.5, 0.75), plot = TRUE)) 

invisible(testOutliers(simulationOutput = m5_sim, alternative  = "two.sided", 
                       margin = "both", type = "binomial", plot = TRUE))

invisible(testDispersion(simulationOutput = m5_sim, alternative = "two.sided", 
                         plot = TRUE, type = "DHARMa"))
```

:::{.callout-tip}
Model 5 assumptions are plausible. The scaled residuals appear to be uniform
both overall (@fig-m5-diagnose-1) and within each threshold with no signs of
heterogeneity of variance (@fig-m5-diagnose-2). There are no outliers
(@fig-m5-diagnose-3), and there is no evidence of over- or under-dispersion
(@fig-m5-diagnose-4).
:::

\FloatBarrier

## Start and Finish Rates for DT Modules
@tbl-DT-module-rates and @fig-DT-module-rates show the estimated start and 
finish rates for each DT module among enrolled applicants who started the 
didactic training. 

```{r}
#| label: tbl-DT-module-rates
#| tbl-cap: Start and Finish Rates By Didactic Training Module Among Enrolled 
#|          Applicants Who Started the Didactic Training

FN <- paste("Missing data are included in the total used as the rate", 
            "denominator. Only Yes values are included in the numerator.",
            "CI, Wilson score confidence interval for a proportion.")

Module_Content <- c("Overview of Forensic Nursing and Sexual Violence",
                    "Victim Responses and Crisis Intervention",
                    "Collaborating with Community Agencies",
                    "Medical-Forensic History Taking",
                    "Observing and Assessing Physical Examination Findings",
                    "Medical-Forensic Specimen Collection",
                    "Medical-Forensic Photography",
                    "Sexually Transmitted Infection Testing and Prophylaxis",
                    "Pregnancy Risk Evaluation and Care",
                    "Medical-Forensic Documentation",
                    "Discharge and Follow-up Planning",
                    "Legal Considerations and Judicial Proceedings")

Module_Rates <- dc_summary(data = StartedDT_Applicants, na.rm = FALSE, 
                          vars = c(paste0("Started_Mod_", 1:12),
                                   paste0("Finished_Mod_", 1:12))) %>% 
  mutate(Module = str_remove(string = Variable, pattern = "Started_Mod_"),
         Module = str_remove(string = Module, pattern = "Finished_Mod_"),
         Module = factor(Module, levels = 1:12),
         Content = factor(Module, levels = 1:12, labels = Module_Content),
         Rate_Type = if_else(str_starts(string = Variable, 
                                        pattern = "Started_Mod_"),
                             true = "Start",
                             false = "Finish"),
         Rate_Type = factor(Rate_Type, levels = c("Start", "Finish"))) %>% 
  mutate(Missing = as.character(Missing),
         No = as.character(No),
         Yes = as.character(Yes),
         Total = as.character(Total)) %>% 
  relocate(Module, Content, Rate_Type) %>% 
  arrange(Module, Rate_Type) %>% 
  select(-Variable) 

Module_Rates %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2, align = "rlrrrrrr",
        format.args = list(nsmall = 2)) %>% 
  kable_styling(font_size = 8) %>% 
  add_header_above(header = c(" " = 3, "Binary Variable (N)" = 4, " ", 
                              "95% CI" = 2)) %>% 
  column_spec(column = 2, width = "6cm") %>% 
  collapse_rows(columns = 1:2, valign = "top", latex_hline = "major",
                row_group_label_position = "first", headers_to_remove = 0) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-DT-module-rates
#| fig-cap: Start and Finish Rates By Didactic Training Module Among Enrolled 
#|          Applicants Who Started the Didactic Training, With 95% Confidence 
#|          Intervals
#| fig-width: 6
#| fig-height: 3

Module_Rates %>% 
  mutate(CI_Label = paste0(sprintf("%.2f", Rate), " [", 
                      sprintf("%.2f", LL), ", ", 
                      sprintf("%.2f", UL), "]")) %>% 
  ggplot(data = ., mapping = aes(y = fct_rev(Module))) + 
  theme_bw() +
  coord_cartesian(xlim = c(0.5, 1.01)) +
  facet_wrap(~ Rate_Type, ncol = 2) + 
  geom_point(aes(x=Rate), size = 1.5) +
  geom_linerange(aes(xmin=LL, xmax=UL)) + 
  geom_text(x = 0.5, aes(label = CI_Label), hjust = 0, cex = 3) +
  labs(x="Rate", y="Module")
```

\FloatBarrier

# Conclusions

## RQ2: What are the attrition rates from enrollment to completion of the CSW, and when (i.e., at which module[s]) did participants commonly attrit?

If we just want to know the overall average attrition rate at each threshold, we
need to examine the main effect of threshold. Model 1 demonstrated that the
attrition rate is not constant throughout the program. Instead, the attrition
rates vary across thresholds and all subsequent models continued to demonstrate 
that fact. 

The simplest answer to RQ1 is presented in @tbl-emmeans-m1 and @fig-emmeans-m1,
but both Model 2 and Model 4 provide better answers. Model 2 output 
(@tbl-emmeans-m2-Both and @fig-emmeans-m2-Both) controls for all focal 
predictors regardless of whether their effects were significant. Model 4 output 
(@tbl-emmeans-m4-Both and @fig-emmeans-m4-Both) only controls for the sole focal 
predictor that mattered. 

## RQ3: Do participants’ background characteristics, motivations for seeking training, potential barriers, and emotional readiness for this work predict attrition rates? 

Of the eight focal predictors we examined, only whether an applicant was 
motivated to seek the training by a need for SANE services in their community or 
organization had a meaningful effect on attrition. The evidence for this lies 
in the single-term deletion tests from Model 2 (@tbl-drop1-m2) and the 
comparison of Models 1 and 4 (@tbl-lrt-m1-m4). 

Applicants who reported having that motivation experienced lower attrition than 
applicants who did not report that motivation. The odds-ratio for this effect in 
Model 2 (@tbl-params-m2) controls for all other focal predictors, while the one 
in Model 4 (@tbl-params-m4) does not. 

Our analyses showed no evidence that the seven remaining focal predictors listed
below exert any influence on attrition from the SANE training program. This is
surprising and counter-intuitive, but is nonetheless what our analyses show.

* Burnout.
* Compassion satisfaction.
* Secondary traumatic stress.
* Family obligations.
* Work responsibilities.
* Primary setting where the trainee practices nursing.
* Motivated to seek the training by a personal connection to sexual assault 
  (e.g., someone they know is a survivor). 

## RQ3: Which focal predictors have parallel effects (constant across thresholds) on attrition and which ones have non-parallel (threshold-specific) effects?
The comparison of Model 2 and 3 (@tbl-lrt-m2-m3), the single-term deletion tests 
from Model 3 (@tbl-drop1-m3) and the comparison of Models 4 and 5 
(@tbl-lrt-m4-m5) all demonstrate that none of the focal predictors have 
non-parallel effects on attrition. 

As noted above in the conclusions regarding RQ2, whether an applicant was 
motivated to seek the training by a need for SANE services in their community or 
organization had a parallel effect on attrition. The evidence for this lies 
in the single-term deletion tests from Model 2 (@tbl-drop1-m2) and the 
comparison of Models 1 and 4 (@tbl-lrt-m1-m4). 

Applicants who reported having that motivation experienced lower attrition than 
applicants who did not report that motivation. The odds-ratio for this effect in 
Model 2 (@tbl-params-m2) controls for all other focal predictors, while the one 
in Model 4 (@tbl-params-m4) does not. 

# References
::: {#refs}
:::

\FloatBarrier

# Software Information
This section documents information that is important for reproducibility. Most
users will not need to read it. It is primarily here for use by the statistician 
on the team if we need to troubleshoot reproducibility issues because someone 
else is unable to get the same results from the same code. Start by checking for 
differences in package versions. 

We used [R](https://www.r-project.org/) as our main computing environment and
[Quarto](https://quarto.org/) scripts to enhance reproducibility. We used
[RStudio](www.rstudio.org) as the editor to interface with R and Quarto.

- Software chain:
  **qmd file > RStudio > Quarto > R > knitr > md file > Pandoc > tex file > TinyTeX > PDF file**.
- Source file: \texttt{\detokenize{`r params$SourceFile`}} 
- Output file: \texttt{\detokenize{`r params$LogFile`}} 
- [Quarto `r quarto_version()`](https://quarto.org/) runs `*.qmd` files through 
  [R](https://www.r-project.org/) and [knitr](https://yihui.org/knitr/) to 
  produce `*.md` markdown files.
- [Pandoc `r rmarkdown::pandoc_version()`](https://pandoc.org) converts markdown 
  files (`*.md`) to other formats, including LaTeX (`*.tex`) and HTML (`*.html`) 
  among others.
- [TinyTeX](https://yihui.org/tinytex/) compiles LaTeX files (`*.tex`) into PDF 
  files. It should be viable to use [MiKTeX](https://miktex.org) or another 
  LaTeX distribution instead. 

\FloatBarrier

## Versions
This document was generated using the following computational environment and 
dependencies:

```{r}
#| label: show-version
#| echo: true

# Check and report whether we used TinyTex or other LaTeX software. 
which_latex()

# Get R and R package version numbers in use.
devtools::session_info()
```

\FloatBarrier

## Git Details
The current Git commit details and status are:

``` {r}
#| label: git-details
#| echo: true

git_report()
```

This is useful because it tells us exactly which commit in the Git history 
we would need to be using to make sure we are running the exact same code. 
Sometimes another person is not using the most current code, or has changed the 
code in some way since it was last committed.  

::: {.callout-tip}
* Untracked files are files located in the repository that Git has not been told 
  to entirely ignore, but have also not been committed into the version history. 
* Unstaged changes to files indicate that some of the contents have been 
  modified since the last time the file was committed to Git. In production 
  runs, we want the Git output to not show any unstaged changes to key files!
  
::: 
