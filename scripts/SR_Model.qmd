---
title: Stopping-Ratio Modeling Results
subtitle: | 
          | CSTAT Case: C1788
          | Clients: Rebecca Campbell, Autumn Ashley, & Katherine Dontje
author: 
  - name: Steven J. Pierce
    orcid: 0000-0002-0679-3019
    email: pierces1@msu.edu
    affil-id: 1 
affiliations: 
  - id: 1 
    name: Michigan State University
    department: Center for Statistical Training and Consulting
    url: "[https://cstat.msu.edu](https://cstat.msu.edu)"
bibliography: references.bib          # File holds BibTeX data for references
csl: apa.csl                          # File controls citation & reference list format
params:                               # Default values for parameters
  SourceDir: "scripts/"               # Relative path to folder holding this file
  SourceFile: "SR_Model.qmd"          # Name of this script file
  LogFile: "SR_Model_Draft.pdf"       # Name of rendered output file
format: 
  pdf:                                # Settings for PDF output
    output-file: "SR_Model_Draft.pdf" # Default output file name 
    output-ext: "pdf"                 # Default file name extension
    documentclass: scrartcl           # LaTeX document type
    papersize: letter
    geometry:                         # Control page margins
      - top=1in
      - bottom=1in
      - left=1in
      - right=1in
    number-sections: true           # Auto-number each section
    toc: true                       # Include table of contents (TOC)
    toc-depth: 3                    # Number of layers in TOC
    colorlinks: true
    template-partials: 
      - title.tex   # Controls author affiliation formatting
    include-in-header:
      - file: compact-title.tex    # Controls spacing around title. 
      - text: |
          \usepackage{fancyhdr}
          \usepackage[noblocks]{authblk}
          \renewcommand*{\Authsep}{, }
          \renewcommand*{\Authand}{, }
          \renewcommand*{\Authands}{, }
          \renewcommand\Affilfont{\small}
          \usepackage[yyyymmdd,hhmmss]{datetime}    %% for currenttime command
          \usepackage{lastpage}                     %% For pageref command
          \usepackage{fontspec}
          \defaultfontfeatures{Ligatures=TeX}
          \usepackage[font={small}, margin=1cm, skip=2pt]{caption}
          \usepackage{url}
          \usepackage{floatrow}                     %% For controlling float placement
          \floatsetup[table]{capposition=top}       %% Puts table caption at top.
          \floatsetup[longtable]{margins=centering} %% Centers longtable tables
          \floatplacement{figure}{!ht}              %% Control placement of figures
          \floatplacement{table}{!ht}               %% Control placement of tables
          \usepackage[labelfont=bf, textfont=bf]{caption}  %% For bolded table/figure captions
          \usepackage{placeins}                     %% For FloatBarrier command
          \usepackage{booktabs}                     %% Used by kableExtra
          \usepackage{longtable}                    %% Used by kableExtra
          \usepackage{array}                        %% Used by kableExtra
          \usepackage{multirow}                     %% Used by kableExtra
          \usepackage{wrapfig}                      %% Used by kableExtra
          \usepackage{colortbl}                     %% Used by kableExtra
          \usepackage{pdflscape}                    %% Used by kableExtra
          \usepackage{tabu}                         %% Used by kableExtra
          \usepackage[normalem]{ulem}               %% Used by kableExtra
          \usepackage{makecell}                     %% Used by kableExtra
          \usepackage{xcolor}                       %% Used by kableExtra
          \usepackage{dcolumn}                      %% Used by kableExtra
          \usepackage{threeparttable}               %% Used by kableExtra
          \usepackage{threeparttablex}              %% Used by kableExtra
          \usepackage{amsmath}                      %% for equation support. 
          \usepackage{titling}      
          \usepackage{verbatim}                     %% For comment command
          \pretitle{\begin{center}\LARGE\bfseries\sffamily}
          \posttitle{\end{center}}
          \pagestyle{fancy}
          \lhead{SANETPA SR Models}
          \chead{\includegraphics[height=0.85cm]{../graphics/Combomark-Horiz_Pantone-567.eps}}
          \rhead{\today\ \currenttime}
          \cfoot{ }
          \fancyfoot[R]{\thepage\ of \pageref*{LastPage}}
          \renewcommand{\headrulewidth}{0.4pt}
          \renewcommand{\footrulewidth}{0.4pt}
          \fancypagestyle{plain}{\pagestyle{fancy}}
          \newcommand*\tocentryformat[1]{{\sffamily#1}}  %% Fix TOC font style
          \RedeclareSectionCommands                      %% Fix TOC font style
            [
              tocentryformat=\tocentryformat,
              tocpagenumberformat=\tocentryformat
            ]
            {section,subsection,subsubsection,paragraph,subparagraph}
execute:                              # Default Quarto chunk execution options
  eval: true
  echo: fenced                        # Show the code in the output file
  output: true
  warning: true
  error: true
  include: true
knitr:                                # Default R knitr package chunk options
  opts_chunk: 
    message: true
    cfsize: "tiny" 
---

\lfoot{\texttt{\small \detokenize{`r params$LogFile`}}}

\FloatBarrier

# Purpose
This file is part of a research compendium [@Pierce-RN8756] associated with a 
study about a sexual assault nurse examiner training program [@Dontje-RN8757]. 
The study aims to document rates of attrition at four points during the 
program and understand predictors of attrition at each of those points. 

This file reads an R data file created by another script in this compendium,
documents some methodology decisions and details, then runs the
stopping-ratio models that comprise the main analysis for the study. This
file also contains some narrative interpretation of the results and
supplementary output such as tables and graphs derived from the models.

# Research Questions
The research questions may be briefly stated as follows:

* **RQ1.** What are the attrition rates from the program at each of the four 
  thresholds representing transitions between stages of program participation?
* **RQ2.** Which focal predictors affect attrition rates? 
    * Burnout.
    * Compassion satisfaction.
    * Secondary traumatic stress.
    * Family obligations.
    * Work responsibilities.
    * Primary setting where the trainee practices nursing.
    * Motivated to seek the training by a need for SANE services in their 
      community or organization.
    * Motivated to seek the training by a personal connection to sexual assault 
      (e.g., someone they know is a survivor). 
* **RQ3.** Which focal predictors have parallel effects (constant across 
  thresholds) on attrition and which ones have non-parallel (threshold-specific) 
  effects?

::: {.callout-tip}
This document uses stopping-ratio models to answer all three research questions 
(RQ1, RQ2, and RQ3). 
:::

::: {.callout-note}
The investigators and the program staff have explicitly decided that the 
following potential predictors are not of substantive interest and should not be
pursued in this paper.

* Trainee satisfaction with the training is not of interest because there is 
  such low variance that it would likely exhibit ceiling effects. 
* Trainee's learning scores in the didactic training and clinical skills 
  workshop are not of interest. 
* Trainee demographics (sex, age, race, etc.) are not of interest because the 
  team wants to focus on analyzing predictors that have more intrinsic meaning 
  and for which there are theoretical reasons to expect effects on attrition. 

:::

\FloatBarrier

# Setup
This section documents some setup tasks that are useful to the statistician on 
the team. Most readers of this document will probably want skip directly to 
@sec-SR-Modeling.

\FloatBarrier

## Define Global Options
Global R chunk options are defined in the YAML header but local chunk options
will over-ride global options. We can temporarily disable an individual chunk by
inserting `#| eval: false` on a line at the top of the chunk. The method for
creating a `cfsize` option that controls font size in code chunks and their text
output is based on an answer to a question posted on
[stackoverflow.com](https://stackoverflow.com/a/46526740).

``` {r}
#| label: global-options

# Create a custom chunk hook/option for controlling font size in chunk & output.
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$cfsize != "normalsize", 
         paste0("\n \\", options$cfsize,"\n\n", x, "\n\n \\normalsize"), 
         x)
  })
```

\FloatBarrier

## Load Packages
R packages usually add new functions to the base R software, allowing you to do 
more things. Here, we load the specific R packages required for this script to 
work.

```{r}
#| label: load-packages
library(here)           # for here(), i_am(), makes code more portable.
library(devtools)       # for session_info()
library(rmarkdown)      # for pandoc_version()
library(knitr)          # for kable()
library(dplyr)          # for %>%, filter(), group_by(), mutate(), rename(), etc.
library(tidyverse)      # for map_dfr(), map_chr(), rowid_to_column(), 
                        # rownames_to_column()
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)     # for kable_styling(), add_header_above(), 
                        # column_spec(), row_spec() etc. 
library(kableExtra)     # for add_header_above(), footnote(), kable_styling()
library(broom)          # for glance(), tidy()
library(car)            # for vif()
library(emmeans)        # for emmeans()
library(performance)    # for model_performance(), r2()
library(scales)         # for label_number()
library(piercer)        # for brier(), display_num(), file_details(), 
                        # git_report(), R2Dev()
library(quarto)         # for quarto_version()
library(SANETPA)        # for version info 
```

\FloatBarrier

## Declare Path
This next chunk declares the path to this script relative to the project-level 
root directory. If the file is not in the right location under the project root
you'll get a warning message. This helps ensure relative paths are all working 
as expected. The chunk below uses the `SourceDir` and `SourceFile` parameters 
set in the YAML header. 

``` {r}
#| label: declare-path

# Declare path to this script relative to the project root directory.
here::i_am(path = paste0(params$SourceDir, params$SourceFile))
```

\FloatBarrier

## Load Data {#sec-Load-Data}
This subsection loads the data created by rendering `scripts/Import_Data.qmd`. 
The data is de-identified to preserve participant privacy and protect 
confidentiality. 

``` {r}
#| label: load-data
#| eval: true

# Store path to data file. 
DataFile <- here("data/Imported_SANETP_Data.RData")

load(file = DataFile)
```

@tbl-imported-data-file shows meta-data about the data file we just loaded and
@tbl-datasets shows the sizes of the datasets it contains. 

```{r}
#| label: tbl-imported-data-file
#| tbl-cap: "Meta-Data About the Data File Loaded"

file_details(DataFile) %>% 
  kable(, format = "latex", booktabs = TRUE, 
        col.names = c("File Name", "Size", "Last Modified")) %>% 
  kable_styling() 
```

```{r}
#| label: tbl-datasets
#| tbl-cap: "Sizes of the Datasets"

data.frame(Dataset = c("Applicants", "Eligible_Applicants", 
                       "Eligible_Applicants_CD", "Thresholds"),
           N_Rows = c(nrow(Applicants), nrow(Eligible_Applicants), 
                      nrow(Eligible_Applicants_CD), nrow(Thresholds)),
           N_Cols = c(ncol(Applicants), ncol(Eligible_Applicants),
                      ncol(Eligible_Applicants_CD), ncol(Thresholds))) %>% 
  kable(, format = "latex", booktabs = TRUE, 
        col.names = c("Dataset", "N Rows", "N Columns")) %>% 
  kable_styling() 
```

\FloatBarrier

# Overview of Stopping-Ratio Modeling {#sec-SR-Modeling}
Attrition from the training program can be conceptualized as a sequential 
filtering process. There are various threshold points between stages of program 
participation where a participant may either attrit or continue participating.
Each of those thresholds is a filter: participants that attrit at a given 
threshold are filtered out of the program. That reduces the number of
participants who reach the next stage of program participation and encounter the
next threshold.

That means an eligible applicant's progress through the training program can be
measured by an ordinal stage variable that records the maximum stage reached by
the applicant in that sequential filtering process. In this study, the ordinal 
variable is called `Stage_Reached`. It is a multinomial ordinal variable with 
$J = 5$ possible stages and observed values denoted by stage $j$, where 
$j \in \{1, 2, 3, 4, 5\}$. 

@fig-Stages shows the final set of stages and thresholds (T1 to T4) between them
at which attrition from the training program could occur. The arrows associated
with each threshold are labeled according to how the outcome variable is coded
on the corresponding person-threshold record, depending on whether the person
stopped participating at the current stage or moved on to the next stage. 

```{dot}
//| label: fig-Stages
//| fig-cap: Stages, Thresholds (T1-T3), and Stopping Ratios (SR1-SR3) in the 
//|          SANE Training Program. CSW, clinical skills workshop; DT, didactic 
//|          training.
//| fig-width: 5
//| fig-height: 2.5

digraph StagesModeled {

graph [rankdir="LR"];

node [shape = "box", style= "filled", fillcolor = "Gray90", fontsize = "7"];
A1 [label = "Attrited\nBefore DT\nSR1"]
A2 [label = "Attrited\nDuring DT\nSR2"]
A3 [label = "Attrited\nBefore/During CSW\nSR3"]

S1 [label = "Stage 1\nEligible\nj = 1"]
S2 [label = "Stage 2\nStarted DT\nj = 2"]
S3 [label = "Stage 3\nFinished DT\nj = 3"]
S4 [label = "Stage 4\nFinished CSW\nj = 4"]

edge [fontsize = "7", arrowsize = 0.5];

S1 -> A1 [label = "T1\nAttrit = 1"]
S2 -> A2 [label = "T2\nAttrit = 1"]
S3 -> A3 [label = "T3\nAttrit = 1"]

S1 -> S2 [label = "Attrit = 0"]
S2 -> S3 [label = "Attrit = 0"]
S3 -> S4 [label = "Attrit = 0"]
}
```

There are several variations of regression models designed to analyze ordinal
outcomes: they test different hypotheses, estimate different parameters, and
yield different insights [@Fullerton-RN8774]. The research questions for this
study are best aligned with a broad class of ordinal regression models usually
called continuation-ratio (CR) models, though they are also called 
stopping-ratio (SR) models or stage models 
[@Fullerton-RN8774; @Liu-RN8772; @Smithson-RN2775; @Yee-RN3711]. The CR model is
ideally suited for ordinal outcome data generated from a sequential selection
process where all individuals start from the same initial stage, must pass
through earlier stages to reach later ones, and all stage transitions are
irreversible [@Fullerton-RN8774; @OConnell-RN8769]. Because our focus is on
predicting attrition--which stops a person's progression to the next stage of
program participation--we will use the SR nomenclature in most of this document
except when referring to broader literature that uses the CR model nomenclature.
It should be understood that CR models and SR models are the same statistical
method. It mostly doesn't matter which which term you use.

@Liu-RN8772 distinguished between forward and backward types of CR models and 
emphasized that each type can be expressed in two different sub-models depending 
on which of the two complementary conditional probabilities for a binary event 
one wants to emphasize (odds versus inversed odds). Regression coefficients 
from the two sub-models of the same type of CR model (forward or backward) have 
the same magnitude but opposite signs. The sub-models for a given type (e.g., 
forward) will also have the same model fit. However, results and interpretation 
will differ between types: the forward and backward CR models are not equivalent 
because they test different hypotheses. This study uses what @Liu-RN8772 called 
a forward CR model of sub-type A. That means we are trying to estimate the 
conditional probability of stopping at stage $j$ conditional on being in or 
above that category given a set of predictors. 

One key to understanding SR models is that they divide the analysis of a single
ordinal outcome into a series of binary outcomes regarding what happens at each
of the thresholds between stages. To run the planned model, we first reorganize
the data from one row per person to one row per person per threshold attempted
[@Cole-RN8770; @Fullerton-RN8774]. That reorganization allows us to use standard
logistic regression modeling software to fit the model and flexibly choose
whether predictor effects are parallel (constrained to equality across
thresholds), or non-parallel (unconstrained and allowed to vary across
thresholds) by either omitting or including interaction terms. Because the 
parallel effect model is nested within the non-parallel effect model, we can 
use likelihood ratio tests to discern whether allowing non-parallel effects 
improves model fit [@Cole-RN8770]. 

The SR model is attractive because its parameters can be translated into 
additional estimates and graphs that are easy to interpret and meaningful for 
assessing the effects of predictors on program attrition. One prior study 
compared CR models to other available models (e.g., logistic regression and 
cumulative odds models for ordinal outcomes) for examining student persistence 
through the remedial math sequence and successfully passing a college-level 
credit bearing math course at 2- and 4-year public institutions 
[@Davidson-RN8773]. @Davidson-RN8773 concluded that CR models provided richer 
insights, in part because they revealed non-parallel effects of some predictors 
that could not be discerned with the other kinds of models. Other studies have 
used CR models to examine predictors of student proficiency in math 
[@Liu-RN8771], to validate student self-assessments of oral language proficiency 
in second language learning [@Winke-RN3795], and examine predictors of how far 
sexual assault kits progressed through several stages of forensic testing 
[@Campbell-RN3126; @Campbell-RN3149; @Campbell-RN3190].

# Methods
Below we fit and examine a series of SR models by using the `glm()` function to 
run logistic regression models on a person-threshold dataset called 
`Thresholds`. This is one way to fit SR models [@Cole-RN8770]. 

The modeling strategy aims to estimate the minimum number of models required to
answer research questions RQ1, RQ2, and RQ3 and obtain a parsimonious final
model. All models use the structural thresholds variable to predict attrition 
because that lays the foundation for a SR model. The modeling sequence starts by 
fitting Model 1, which only uses the structural thresholds variable as a 
predictor. That is the baseline model we can compare with models containing the
focal predictors. Model 2 adds a set of parallel effects for all seven focal
predictors, then Model 3 expands on Model 2 by adding interaction terms to
estimate non-parallel effects for all seven focal predictors [@Cole-RN8770].

Models 4 and 5 aim to achieve parsimony by only including the one focal
predictor that demonstrated an effect in Model 2. Model 4 builds up from Model 1
by adding a parallel effect for that focal predictor, then Model 5 further 
extends the analysis to test whether it had a non-parallel effect. 

We examine model diagnostics and fit statistics, model comparisons via 
likelihood ratio tests, single-term deletion tests based on Type II sums of 
squares, and other supplementary output. Effect sizes are quantified by 
odds-ratios and estimated marginal means for the attrition rates reported on a 
probability scale. 

We are using listwise deletion to handle missing data because only 3 of 327
(0.9%) cases were incomplete (see the output from the data import script). These 
cases would have to be very influential for omitting them to induce substantial 
bias in the model results. When more than 5% of cases have missing data, more
sophisticated approaches may be required [@Fernández-García-RN4151], but this
dataset is well below that threshold.

## Sample
The sample consists of the eligible applicants to the program who had complete 
data on all the measures listed in the next section. We aim to draw conclusions 
that generalize to the overall population of eligible applicants to the program. 

## Measures
The measures mentioned below include an outcome variable, a structural predictor
required for SR modeling, and eight focal predictors.

### Attrition (Outcome Variable)
This is the binary, threshold-specific transformation of the `Stage_Reached` 
ordinal outcome variable. It is the outcome modeled in our SR models. The 
variable `Attrit` is coded 1 when an applicant attrited, and 0 when they passed 
the threshold and continued to the next stage of the program. See @fig-Stages 
for a visualization of the coding for `Attrit`. 

### Threshold (Structural Predictor)
The `Threshold` variable identifies which of four thresholds a participant was 
attempting on any given record in the `Threshods` dataset. See @fig-Stages for 
an overview of the stages. 

### Burnout (Focal Predictor)
This is a mean-centered, continuous variable (`CProQOL_BO`) derived from the 
applicants score on the ProQOL burnout scale [@Stamm-RN8775]. 

### Compassion Satisfaction (Focal Predictor)
This is a mean-centered, continuous variable (`CProQOL_CS`) derived from the 
applicants score on the ProQOL compassion satisfaction scale [@Stamm-RN8775]. 

### Secondary Traumatic Stress (Focal Predictor)
This is a mean-centered, continuous variable (`CProQOL_STS`) derived from the 
applicants score on the ProQOL secondary traumatic stress scale [@Stamm-RN8775]. 

### Barrier: Family Obligations (Focal Predictor)
This is a mean-centered, continuous variable (`Barrier_FO`) measured by a single 
Likert-response item that was scored on a 5-point scale. See the descriptive 
analyses output file for this study for more details. 

### Barrier: Work Responsibilities (Focal Predictor)
This is a mean-centered, continuous variable (`Barrier_WR`) measured by a single 
Likert-response item that was scored on a 5-point scale. See the descriptive 
analyses output file for this study for more details. 

### Setting (Focal Predictor)
The primary setting where the trainee practices nursing was a nominal
categorical factor named `Setting`. It originally had four levels, but was
recoded to three levels (`Urban`, `Rural/Tribal`, and `Suburban`) due to small
sample size in the `Tribal` category (which was then combined with `Rural`). See
the descriptive analyses output file for this study for more details. The
reference level is`Urban`.

### Motivated by Need for SANE (Focal Predictor)  
This is a binary variable (`Motivation_NeedSANE`) measuring whether the 
applicant reported being motivated to seek the training by a need for SANE 
services in their community or organization. The reference level is `No`. 

### Motivated by Personal Connection (Focal Predictor)
This is a binary variable (`Motivation_PersonalConn`) measuring whether the 
to seek the training by a personal connection to sexual assault (e.g., someone 
they know is a survivor). The reference level is `No`. 

\FloatBarrier

# Results

\FloatBarrier

## Model 1: Thresholds Alone
The modeling starts by first fitting a very simple model (`m1`) containing only 
a threshold main effect. This should allow is to see whether attrition rate 
varies across the thresholds. @tbl-params-m1 shows the model parameters and 
@tbl-fit-m1 shows its fit statistics. 

```{r}
#| label: tbl-params-m1
#| tbl-cap: "Model 1 Parameters"

m1 <- glm(Attrit ~ Threshold. + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m1) %>% 
  cbind(confint.default(m1)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2))
```

```{r}
#| label: tbl-fit-m1
#| tbl-cap: "Model 1 Fit Statistics"

glance(m1) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Single Term Deletion Tests
@tbl-drop1-m1 shows the effect of deleting the threshold term from the model to 
determine whether doing so worsens model fit. A significant effect should be 
retained in the model because omitting it would damage the model fit.

```{r}
#| label: tbl-drop1-m1
#| tbl-cap: "Model 1 Single Term Deletion Tests (Type II SS)"
#| warning: false

m1 %>% 
  drop1(., test = "LRT") %>% 
  tidy() %>% 
  rename(p = p.value) %>% 
  mutate(p = display_num(p)) %>% 
  kable(format = "latex", booktabs = TRUE,
        digits = c(0, 0, 2, 2, 2, Inf),
        col.names = c("Term Deleted", "df", "Deviance", "AIC", "LRT", "p")) %>% 
  kable_styling() 
```

::: {.callout-tip}
The threshold main effect is integral to fitting a stopping-ratio model. It must
remain in the model to properly test for both parallel and non-parallel effects
of focal predictors. Removing the threshold main effect would also harm model
fit, so we will keep it in all subsequent models.
:::

\FloatBarrier

### Estimated Marginal Means
The presence of a threshold main effect means that the attrition rate varies
across thresholds. Next we compute estimated marginal means (EMMs) for each
threshold and back-transform them to the response (probability) scale to see the
attrition rate at each threshold. We expect these rates to agree with the simple
descriptive statistics for percent attrition at each threshold because threshold
is the sole predictor in Model 1. @tbl-emmeans-m1 shows the EMMs derived from
Model 1.

```{r}
#| label: tbl-emmeans-m1
#| tbl-cap: "Model 1 Conditional Attrition Rate by Threshold"

FN <- paste("Values are on the probability scale.",
            "EMM, estimated marginal mean.")

emmeans(m1, specs = ~ Threshold., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Threshold", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m1
#| fig-cap: "Model 1 Conditional Attrition Rate by Threshold (Main Effect)"
#| fig-width: 4
#| fig-height: 2.5

emmeans(m1, specs = ~ Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Threshold") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

\FloatBarrier

## Model 2: Thresholds & Parallel Effects for All Focal Predictors
In SR models, a predictor has a parallel effect when its regression coefficient 
is constrained equal across all thresholds. This is implemented by estimating 
only a main effect for the predictor (it cannot interact with the threshold). 
Model 2 (`m2`) uses parallel effects for the focal predictors. See 
@tbl-params-m2 for parameter estimates and @tbl-fit-m2 for the fit statistics. 

```{r}
#| label: tbl-params-m2
#| tbl-cap: "Model 2 Parameters"

m2 <- glm(Attrit ~ Threshold. + CBarrier_FO + CBarrier_WR + CProQOL_BO + 
            CProQOL_CS + CProQOL_STS + Motivation_NeedSANE. + 
            Motivation_PersonalConn. + Setting. + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m2) %>% 
  cbind(confint.default(m2)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2))
```

```{r}
#| label: tbl-fit-m2
#| tbl-cap: "Model 2 Fit Statistics"

glance(m2) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Check for Multicollinearity
We will check for multicollinearity problems by examining the generalized
variance inflation factor (GVIF) and a transformation of it that adjusts for
categorical terms that use multiple degrees of freedom [@Fox-RN3484]. In 
general, one can compare to $GVIF^{(1/(2*df))}$ to cutoffs defined for 
$\sqrt{VIF}$. Some suggested cutoffs indicating potential problems are 
$\sqrt{VIF} \ge 2$ or $\sqrt{VIF} \ge 3$. We are hoping to see 
$GVIF^{(1/(2*df))} < 3$. 

```{r}
#| label: tbl-GVIF-m2
#| tbl-cap: Model 2 Generalized Variance Inflation Factors for Predictors

FN <- paste("Values >= 3 in the last column indicate multicollinearity problems.")

car::vif(m2) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Term") %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

::: {.callout-tip}
We do not appear to have any multicollinearity problems in Model 2.
:::

\FloatBarrier

### Compare Models 1 and 2
@tbl-lrt-m1-m2 shows a likelihood ratio test (LRT) comparing Models 1 and 2. 

```{r}
#| label: tbl-lrt-m1-m2
#| tbl-cap: "Likehood Ratio Test Comparing Models 1 and 2"

test_lrt(m1, m2) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m1", 
                         true = "Thesholds", 
                         false = "Thresholds + All Parallel Effects")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

\FloatBarrier

::: {.callout-caution}
This is a simultaneous test of whether adding parallel effects for all of the
focal predictors as a block improved the model fit enough to justify adding that
many extra parameters. It did not. However, the Wald tests in @tbl-params-m2 
suggest that `Motivation_NeedSANE.` has an effect while none of the other
predictors do. We need to look more closely at the effects of individual
predictors via single-term deletion tests.
:::

\FloatBarrier

### Single Term Deletion Tests
@tbl-drop1-m2 shows the effect of deleting specific terms from the model to 
determine whether doing so worsens model fit. Each main effect term is tested 
after controlling for all the other main effects. This is a more focused test 
of individual predictors than the overall LRT comparing Models 1 and 2. 
Significant effects should be retained in the model because omitting them would 
damage the model fit. Non-significant effects can potentially be omitted. 

```{r}
#| label: tbl-drop1-m2
#| tbl-cap: "Model 2 Single Term Deletion Tests (Type II SS)"
#| warning: false

m2 %>% 
  drop1(., test = "LRT") %>% 
  tidy() %>% 
  rename(p = p.value) %>% 
  mutate(p = display_num(p)) %>% 
  kable(format = "latex", booktabs = TRUE,
        digits = c(0, 0, 2, 2, 2, Inf),
        col.names = c("Term Deleted", "df", "Deviance", "AIC", "LRT", "p")) %>% 
  kable_styling() 
```

\FloatBarrier

::: {.callout-tip}
Only `Threshold` and `Motivation_NeedSANE` have main effects the need to be 
retained. We could in theory remove all the other focal predictors. 
:::

\FloatBarrier

### Estimated Marginal Means
Next we compute estimated marginal means (EMMs) for each threshold and 
back-transform them to the response (probability) scale to see the attrition 
rate at each threshold. @tbl-emmeans-m2-Threshold and @fig-emmeans-m2-Threshold 
show the EMMs derived from Model 2 for each threshold, while 
@tbl-emmeans-m2-NeedSANE and @fig-emmeans-m2-NeedSANE show the EMMs for each 
level of `Motivation_NeedSANE`. @tbl-emmeans-m2-Both and @fig-emmeans-m2-Both 
show the combined result of the two main effects while averaging over the
results of all other predictors.

```{r}
#| label: tbl-emmeans-m2-Threshold
#| tbl-cap: Model 2 Conditional Attrition Rate by Threshold (Main Effect)

FN <- paste("Values are on the probability scale.", 
            "Results are averaged over the levels of all other categorical", 
            "predictors and assume values at the mean on continuous focal", 
            "predictors.",
            "EMM, estimated marginal mean.")

emmeans(m2, specs = ~ Threshold., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Threshold", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m2-Threshold
#| fig-cap: Model 2 Conditional Attrition Rate by Threshold (Main Effect 
#|          Averaged Over All Other Predictors)
#| fig-width: 4
#| fig-height: 2.5

emmeans(m2, specs = ~ Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Threshold") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

```{r}
#| label: tbl-emmeans-m2-NeedSANE
#| tbl-cap: Model 2 Conditional Attrition Rate by Motivation_NeedSANE (Main 
#|          Effect)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Values are on the probability scale.",
            "Results are averaged over the levels of all other categorical", 
            "predictors and assume values at the mean on continuous focal", 
            "predictors.",
            "EMM, estimated marginal mean.")

emmeans(m2, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Motivation_NeedSANE.", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m2-NeedSANE
#| fig-cap: Model 2 Conditional Attrition Rate by Motivation_NeedSANE (Parallel 
#|          Main Effect Averaged Over All Other Predictors)
#| fig-width: 2
#| fig-height: 2.5

emmeans(m2, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

```{r}
#| label: tbl-emmeans-m2-Both
#| tbl-cap: Model 2 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Results are averaged over the levels of all other categorical", 
            "predictors and assume values at the mean on continuous focal", 
            "predictors.",
            "Values are on the probability scale.", 
            "EMM, estimated marginal mean.")

emmeans(m2, specs = ~ Threshold. | Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  arrange(Threshold., Motivation_NeedSANE.) %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 0, 3, 3, 3, 3),
        col.names = c("Threshold", "Motivation_NeedSANE.", "Rate (EMM)", "SE", 
                      "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 4, "95% Wald CI" = 2)) %>% 
  collapse_rows(columns = 1:2, valign = "top", latex_hline = "major", 
                custom_latex_hline = 1, headers_to_remove = 0,
                row_group_label_position = "stack") %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m2-Both
#| fig-cap: Model 2 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects, Averaged Over All Other 
#|          Predictors)
#| fig-width: 4.5
#| fig-height: 2.5

emmeans(m2, specs = ~ Motivation_NeedSANE. | Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  facet_grid(cols = vars(Threshold.), labeller = label_both) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

\FloatBarrier

## Model 3: Thresholds & Non-Parallel Effects for All Focal Predictors
In SR models, a predictor has a non-parallel effect when its regression 
coefficient is allowed to vary across all thresholds. That is implemented by 
allowing the predictor to have both a main effect and an interaction with the 
threshold. Continuous predictors must be appropriately centered before model 
fitting as usual when testing interactions (i.e., moderator hypotheses). Model 3 
(`m3`) treats threshold as a moderator of the other predictors' effects. See 
@tbl-params-m3 for parameter estimates and @tbl-fit-m3 for fit statistics. 

We are continuing to include predictors that did not have significant main 
effects in Model 2 because it is possible that threshold moderates the effects 
of other predictors such that there are non-parallel (threshold-specific) 
effects that get canceled out when averaging across them to estimate a parallel 
main effect. Thus, our approach maximizes the chance that we detect any 
unusually nuanced non-parallel effects if they exist. 

::: {.callout-caution}
Model 3 is rather ambitious given the sample size we have available. It is
adding seven interaction effects: that yields many extra parameters. This poses
some risk for over-fitting the model, but testing for non-parallel effects
either requires fitting this one model, or fitting a set of seven other models
where we add only a single non-parallel effect at a time. Fitting this larger
model is more efficient and reduces the total amount of output we have to
review.
:::

```{r}
#| label: tbl-params-m3
#| tbl-cap: "Model 3 Parameters"

m3 <- glm(Attrit ~ Threshold. + CBarrier_FO + CBarrier_WR + CProQOL_BO + 
            CProQOL_CS + CProQOL_STS + Motivation_NeedSANE. + 
            Motivation_PersonalConn. + Setting. + Threshold.:CBarrier_FO +
            Threshold.:CBarrier_WR + Threshold.:CProQOL_BO + 
            Threshold.:CProQOL_CS + Threshold.:CProQOL_STS + 
            Threshold.:Motivation_NeedSANE. + 
            Threshold.:Motivation_PersonalConn. + Threshold.:Setting. + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m3) %>% 
  cbind(confint.default(m3)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(Parameter = str_replace(string = Parameter, pattern = ":", 
                                 replacement = " x "), 
         p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling(font_size = 10) %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2)) %>% 
  column_spec(column = 1, width = "5.5cm")
```

```{r}
#| label: tbl-fit-m3
#| tbl-cap: "Model 3 Fit Statistics"

glance(m3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Check for Multicollinearity
@tbl-GVIF-m3 shows the generalized variance inflation factors (GVIF) for Model 
3. 

```{r}
#| label: tbl-GVIF-m3
#| tbl-cap: Model 3 Generalized Variance Inflation Factors for Predictors
#| warning: false

FN <- paste("Values >= 3 in the last column indicate multicollinearity problems.")

car::vif(m3) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Term") %>% 
  mutate(Term = str_replace(string = Term, pattern = ":", 
                            replacement = " x ")) %>%  
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

::: {.callout-tip}
We do not appear to have any multicollinearity problems in Model 3
:::

\FloatBarrier

### Compare Models 2 and 3
@tbl-lrt-m2-m3 shows a likelihood ratio test (LRT) comparing Models 2 and 3. 

```{r}
#| label: tbl-lrt-m2-m3
#| tbl-cap: "Likehood Ratio Test Comparing Models 2 and 3"

test_lrt(m2, m3) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m2", 
                         true = "Thesholds + All Parallel Effects", 
                         false = "Thresholds + All Non-Parallel Effects")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

::: {.callout-caution}
This is a simultaneous test of whether adding non-parallel effects for all of 
the focal predictors as a block improved the model fit enough to justify adding 
that many extra parameters. Apparently it did not. We should still look more 
closely at the effects of individual predictors via single-term deletion tests.
:::

\FloatBarrier

### Single Term Deletion Tests
@tbl-drop1-m3 shows the effect of deleting specific interaction terms from the 
model to determine whether doing so worsens model fit. Significant effects 
should be retained in the model because omitting them would damage the model 
fit. We do not test main effects for predictors involved in interactions because 
we need to respect the principle of marginality 
[@Fox-RN3484; @Hector-RN3472; @Langsrud-RN1287]. 

```{r}
#| label: tbl-drop1-m3
#| tbl-cap: "Model 3 Single Term Deletion Tests (Type II SS)"
#| warning: false

m3 %>% 
  drop1(., test = "LRT") %>% 
  tidy() %>% 
  rename(p = p.value) %>% 
  mutate(term = str_replace(string = term, pattern = ":", 
                            replacement = " x "), 
         p = display_num(p)) %>% 
  kable(format = "latex", booktabs = TRUE,
        digits = c(0, 0, 2, 2, 2, Inf),
        col.names = c("Term Deleted", "df", "Deviance", "AIC", "LRT", "p")) %>% 
  kable_styling() 
```

::: {.callout-tip}
We can safely drop all of the interaction effects from the model. None of them 
improve the model fit. It may be worth trying a final pair of models that are 
more parsimonious than Models 2 and 3: they should include models with threshold 
effect and just parallel and non-parallel effects of `Motivation_NeedSANE` 
without other focal predictors. 
:::

\FloatBarrier

## Model 4: Thresholds & Parallel Effect for `Motivation_NeedSANE`
Model 4 uses a parallel effect for `Motivation_NeedSANE` without any of the 
other focal predictors. @tbl-params-m4 shows the parameter estimates and 
@tbl-fit-m4 shows the fit indices. 

```{r}
#| label: tbl-params-m4
#| tbl-cap: "Model 4 Parameters"

m4 <- glm(Attrit ~ Threshold. + Motivation_NeedSANE. + 1 , 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m4) %>% 
  cbind(confint.default(m4)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2))
```

```{r}
#| label: tbl-fit-m4
#| tbl-cap: "Model 4 Fit Statistics"

glance(m4) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Compare Models 1 and 4
@tbl-lrt-m1-m2 shows a likelihood ratio test (LRT) comparing Models 1 and 4. 

```{r}
#| label: tbl-lrt-m1-m4
#| tbl-cap: "Likehood Ratio Test Comparing Models 1 and 4"

test_lrt(m1, m4) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m1", 
                         true = "Thesholds", 
                         false = "Thresholds + Parallel Motivation_NeedSANE Effect")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

\FloatBarrier

### Estimated Marginal Means
Next we compute estimated marginal means (EMMs) for each threshold and 
back-transform them to the response (probability) scale to see the attrition 
rate at each threshold. @tbl-emmeans-m4-Threshold and @fig-emmeans-m4-Threshold 
show the EMMs derived from Model 4 for each threshold, while 
@tbl-emmeans-m4-NeedSANE and @fig-emmeans-m4-NeedSANE show the EMMs for each 
level of `Motivation_NeedSANE`. @tbl-emmeans-m4-Both and @fig-emmeans-m4-Both 
show the combined result of the two main effects .

```{r}
#| label: tbl-emmeans-m4-Threshold
#| tbl-cap: Model 4 Conditional Attrition Rate by Threshold (Main Effect)

FN <- paste("Values are on the probability scale.", 
            "Results are averaged over the levels of Motivation_NeedSANE.",
            "EMM, estimated marginal mean.")

emmeans(m4, specs = ~ Threshold., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Threshold", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m4-Threshold
#| fig-cap: Model 4 Conditional Attrition Rate by Threshold (Main Effect 
#|          Averaged Over Motivation_NeedSANE)
#| fig-width: 4
#| fig-height: 2.5

emmeans(m4, specs = ~ Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Threshold") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

```{r}
#| label: tbl-emmeans-m4-NeedSANE
#| tbl-cap: Model 4 Conditional Attrition Rate by Motivation_NeedSANE (Main 
#|          Effect)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Values are on the probability scale.",
            "Results are averaged over the levels of Threshold.",
            "EMM, estimated marginal mean.")

emmeans(m4, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 3, 3, 3, 3),
        col.names = c("Motivation_NeedSANE.", "Rate (EMM)", "SE", "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2)) %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m4-NeedSANE
#| fig-cap: Model 4 Conditional Attrition Rate by Motivation_NeedSANE (Parallel 
#|          Main Effect Averaged Over Threshold)
#| fig-width: 2
#| fig-height: 2.5

emmeans(m4, specs = ~ Motivation_NeedSANE., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

```{r}
#| label: tbl-emmeans-m4-Both
#| tbl-cap: Model 4 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects)

FN <- paste("Motivation_NeedSANE has a parallel effect on attrition.",
            "Values are on the probability scale.", 
            "EMM, estimated marginal mean.")

emmeans(m4, specs = ~ Threshold. | Motivation_NeedSANE., regrid = "response") %>% 
  as_tibble() %>% 
  arrange(Threshold., Motivation_NeedSANE.) %>% 
  select(!df) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(0, 0, 3, 3, 3, 3),
        col.names = c("Threshold", "Motivation_NeedSANE.", "Rate (EMM)", "SE", 
                      "LL", "UL")) %>% 
  kable_styling() %>% 
  add_header_above(header = c(" " = 4, "95% Wald CI" = 2)) %>% 
  collapse_rows(columns = 1:2, valign = "top", latex_hline = "major", 
                custom_latex_hline = 1, headers_to_remove = 0,
                row_group_label_position = "stack") %>% 
  footnote(kable_input = ., general = FN, footnote_as_chunk = TRUE, 
           threeparttable = TRUE)
```

```{r}
#| label: fig-emmeans-m4-Both
#| fig-cap: Model 4 Conditional Attrition Rate by Threshold and 
#|          Motivation_NeedSANE (Main Effects Combined)
#| fig-width: 4.5
#| fig-height: 2.5

emmeans(m4, specs = ~ Motivation_NeedSANE. | Threshold., regrid = "response") %>% 
  plot(horizontal = FALSE) +
  facet_grid(cols = vars(Threshold.), labeller = label_both) +
  xlab("Attrition Rate") + 
  ylab("Motivation_NeedSANE") + 
  scale_x_continuous(lim = c(0.00, 0.50), labels = label_number(accuracy = 0.01))
```

\FloatBarrier

## Model 5: Thresholds & Non-Parallel Effect for `Motivation_NeedSANE` 
Model 5 uses a non-parallel effect for `Motivation_NeedSANE` without any of the 
other focal predictors. @tbl-params-m5 shows the parameter estimates and 
@tbl-fit-m5 shows the fit indices. 

```{r}
#| label: tbl-params-m5
#| tbl-cap: "Model 5 Parameters"

m5 <- glm(Attrit ~ Threshold. + Motivation_NeedSANE. + 
          Threshold.:Motivation_NeedSANE. + 1, 
          data = Thresholds, family = binomial(link = "logit"))

FN <- paste("Values shown are on the link function (logit) scale.")

tidy(m5) %>% 
  cbind(confint.default(m5)) %>% 
  as_tibble() %>% 
  rename(Parameter = term, Est = estimate, SE = std.error, z = statistic, 
         p = p.value, LL = `2.5 %`, UL = `97.5 %`) %>% 
  mutate(Parameter = str_replace(string = Parameter, pattern = ":", 
                                 replacement = " x "), 
         p = display_num(p),
         OR = exp(Est), 
         OR.LL = exp(LL), 
         OR.UL = exp(UL)) %>% 
  relocate(Parameter, Est, SE, LL, UL, OR, OR.LL, OR.UL, z, p) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 2) %>% 
  kable_styling(font_size = 10) %>% 
  add_header_above(header = c(" " = 3, "95% Wald CI" = 2, " " = 1, 
                              "OR 95% Wald CI" = 2, "Wald Test" =  2)) %>% 
  column_spec(column = 1, width = "5.5cm")
```

```{r}
#| label: tbl-fit-m5
#| tbl-cap: "Model 5 Fit Statistics"

glance(m5) %>% 
  kable(format = "latex", booktabs = TRUE, 
        digits = c(2, 0, 2, 2, 2, 2, 0, 0)) %>% 
  kable_styling()
```

\FloatBarrier

### Compare Models 4 and 5
@tbl-lrt-m4-m5 shows a likelihood ratio test (LRT) comparing Models 4 and 5. 

```{r}
#| label: tbl-lrt-m4-m5
#| tbl-cap: "Likehood Ratio Test Comparing Models 4 and 5"

test_lrt(m4, m5) %>% 
  as_tibble() %>% 
  mutate(p = display_num(p),
         Model = if_else(Name == "m4", 
                         true = "Thesholds + Parallel Effect of Motivation_NeedSANE", 
                         false = "Thresholds + Non-Parallel Effect of Motivation_NeedSANE")) %>% 
  #select(-term) %>% 
  kable(format = "latex", booktabs = TRUE, digits = c(0, 0, 0, 0, 2, 3),
        col.names = c("Model", "Terms", "df", "df_diff", "LRT", "p")) %>% 
  kable_styling()
```

::: {.callout-tip}
This tests whether adding a non-parallel effect for `Motivation_NeedSANE` 
improved the model fit enough to justify adding that many extra parameters. It 
did not. We can safely drop the interaction effect from the model. That tells us
we that if we're comfortable pursuing a more parsimonious model than Model 2, we
should interpret Model 4 instead of Model 5.
:::

\FloatBarrier

# Conclusions

## RQ1: What are the attrition rates from the program at each of the four thresholds representing transitions between stages of program participation?

If we just want to know the overall average attrition rate at each threshold, we
need to examine the main effect of threshold. Model 1 demonstrated that the
attrition rate is not constant throughout the program. Instead, the attrition
rates vary across thresholds and all subsequent models continued to demonstrate 
that fact. 

The simplest answer to RQ1 is presented in @tbl-emmeans-m1 and @fig-emmeans-m1,
but both Model 2 and Model 4 provide better answers. Model 2 output 
(@tbl-emmeans-m2-Both and @fig-emmeans-m2-Both) controls for all focal 
predictors regardless of whether their effects were significant. Model 4 output 
(@tbl-emmeans-m4-Both and @fig-emmeans-m4-Both) only controls for the sole focal 
predictor that mattered. 

## RQ2: Which focal predictors affect attrition rates? 
Of the seven focal predictors we examined, only whether an applicant was 
motivated to seek the training by a need for SANE services in their community or 
organization had a meaningful effect on attrition. The evidence for this lies 
in the single-term deletion tests from Model 2 (@tbl-drop1-m2) and the 
comparison of Models 1 and 4 (@tbl-lrt-m1-m2). 

Applicants who reported having that motivation experienced lower attrition than 
applicants who did not report that motivation. The odds-ratio for this effect in 
Model 2 (@tbl-params-m2) controls for all other focal predictors, while the one 
in Model 4 (@tbl-params-m4) does not. 

Our analyses showed no evidence that the seven remaining focal predictors listed
below exert any influence on attrition from the SANE training program. This is
surprising and counter-intuitive, but is nonetheless what our analyses show.

* Burnout.
* Compassion satisfaction.
* Secondary traumatic stress.
* Family obligations.
* Work responsibilities.
* Primary setting where the trainee practices nursing.
* Motivated to seek the training by a personal connection to sexual assault 
  (e.g., someone they know is a survivor). 

## RQ3: Which focal predictors have parallel effects (constant across thresholds) on attrition and which ones have non-parallel (threshold-specific) effects?
The comparison of Model 2 and 3 (@tbl-lrt-m2-m3), the single-term deletion tests 
from Model 3 (@tbl-drop1-m3) and the comparison of Models 4 and 5 
(@tbl-lrt-m4-m5) all demonstrate that none of the focal predictors have 
non-parallel effects on attrition. 

As notes above in the conclusions regarding RQ2, whether an applicant was 
motivated to seek the training by a need for SANE services in their community or 
organization had a parallel effect on attrition. The evidence for this lies 
in the single-term deletion tests from Model 2 (@tbl-drop1-m2) and the 
comparison of Models 1 and 4 (@tbl-lrt-m1-m2). 

Applicants who reported having that motivation experienced lower attrition than 
applicants who did not report that motivation. The odds-ratio for this effect in 
Model 2 (@tbl-params-m2) controls for all other focal predictors, while the one 
in Model 4 (@tbl-params-m4) does not. 

# References
::: {#refs}
:::

\FloatBarrier

# Software Information
This section documents information that is important for reproducibility. Most
users will not need to read it. It is primarily here for use by the statistician 
on the team if we need to troubleshoot reproducibility issues because someone 
else is unable to get the same results from the same code. Start by checking for 
differences in package versions. 

We used [R](https://www.r-project.org/) as our main computing environment and
[Quarto](https://quarto.org/) scripts to enhance reproducibility. We used
[RStudio](www.rstudio.org) as the editor to interface with R and Quarto.

- Software chain:
  **qmd file > RStudio > Quarto > R > knitr > md file > Pandoc > tex file > TinyTeX > PDF file**.
- Source file: \texttt{\detokenize{`r params$SourceFile`}} 
- Output file: \texttt{\detokenize{`r params$LogFile`}} 
- [Quarto `r quarto_version()`](https://quarto.org/) runs `*.qmd` files through 
  [R](https://www.r-project.org/) and [knitr](https://yihui.org/knitr/) to 
  produce `*.md` markdown files.
- [Pandoc `r rmarkdown::pandoc_version()`](https://pandoc.org) converts markdown 
  files (`*.md`) to other formats, including LaTeX (`*.tex`) and HTML (`*.html`) 
  among others.
- [TinyTeX](https://yihui.org/tinytex/) compiles LaTeX files (`*.tex`) into PDF 
  files. It should be viable to use [MiKTeX](https://miktex.org) or another 
  LaTeX distribution instead. 

\FloatBarrier

## Versions
This document was generated using the following computational environment and 
dependencies:

```{r}
#| label: show-version
#| echo: true

# Check and report whether we used TinyTex or other LaTeX software. 
which_latex()

# Get R and R package version numbers in use.
devtools::session_info()
```

\FloatBarrier

## Git Details
The current Git commit details and status are:

``` {r}
#| label: git-details
#| echo: true

git_report()
```

This is useful because it tells us exactly which commit in the Git history 
we would need to be using to make sure we are running the exact same code. 
Sometimes another person is not using the most current code, or has changed the 
code in some way since it was last committed.  

::: {.callout-tip}
* Untracked files are files located in the repository that Git has not been told 
  to entirely ignore, but have also not been committed into the version history. 
* Unstaged changes to files indicate that some of the contents have been 
  modified since the last time the file was committed to Git. In production 
  runs, we want the Git output to not show any unstaged changes to key files!
  
::: 
